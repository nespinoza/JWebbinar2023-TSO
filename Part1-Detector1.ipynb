{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e21d550",
   "metadata": {},
   "source": [
    "<img style=\"float: center;\" src='https://github.com/STScI-MIRI/MRS-ExampleNB/raw/main/assets/banner1.png' alt=\"stsci_logo\" width=\"1000px\"/> \n",
    "\n",
    "# TSO JWebbinar Notebook 1: Downloading and Calibrating `uncal` TSO Products\n",
    "-----\n",
    "\n",
    "**Author**: NÃ©stor Espinoza | AURA Assistant Astronomer | Mission Scientist for Exoplanet Science\n",
    "<br>\n",
    "**Last Updated**: December 7th, 2023\n",
    "<br>\n",
    "**Pipeline Version**: 1.12.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc24f5c",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Introduction](#intro)<br>\n",
    "   1.1 [Purpose of this Notebook](#purpose)<br>\n",
    "   1.2 [Data & Context of the Observations](#data)<br>\n",
    "2. [Downloading & Quick Looks at JWST TSO data](#download)<br>\n",
    "   2.1 [Downloading TSO data from MAST](#mast)<br>\n",
    "   2.2 [Quicklook, pt. I: Target Acquisition](#ta)<br>\n",
    "   2.3 [Quicklook, pt. II: `datamodels` & TSO Science Data Products](#science)<br>\n",
    "4. [A TSO tour through the `Detector1` stage](#detector1)<br>\n",
    "   3.1 [Checking data quality flags](#dqflags)<br>\n",
    "   3.2 [Identifying saturated pixels](#saturation)<br>\n",
    "   3.3 [Removing detector-level effects: the `superbias` and `refpix` steps](#refpix)<br>\n",
    "   3.4 [Linearity corrections](#linearity)<br>\n",
    "   3.5 [Removing the dark current](#darkcurrent)<br>\n",
    "   3.6 [Handling 1/f noise at the group-level](#1overf)<br>\n",
    "   3.7 [Detecting \"jumps\" on up-the-ramp sampling](#jump)<br>\n",
    "   3.8 [Fitting ramps with the `ramp_fit` step](#rampfit)<br>\n",
    "6. [Final words](#final-words)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0a705b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "1.<font color='white'>-</font>Introduction <a class=\"anchor\" id=\"intro\"></a>\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffeef35",
   "metadata": {},
   "source": [
    "### 1.1<font color='white'>-</font>Purpose of this Notebook<a class=\"anchor\" id=\"purpose\"></a> ###\n",
    "\n",
    "In this Notebook, we aim to perform an exploration of Time Series Observations (TSO) products, focusing in particular on products obtained by the [Transiting Exoplanet JWST Early Release Science (ERS) team](https://www.stsci.edu/jwst/science-execution/approved-programs/dd-ers/program-1366) --- a real science dataset that we will reduce starting from the most \"raw\" forms of data products that can be downloaded from MAST. We will learn how to download those products, as well as how to load them and make them interact with the JWST Calibration Pipeline to calibrate them. In a companion Notebook, we then perform spectroscopic analyses on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc00ab4",
   "metadata": {},
   "source": [
    "### 1.2<font color='white'>-</font>Data & Context of the Observations<a class=\"anchor\" id=\"data\"></a> ###\n",
    "\n",
    "The input data for this Notebook are observations from the Transiting Exoplanet ERS team; in particular, observations of the exoplanet WASP-39b obtained with the JWST NIRSpec/G395H mode. This mode stores data in two detectors, NRS1 and NRS2 which hold data from about 3-4 $\\mu$m and 4-5 $\\mu$m, respectively. Results from those observations were presented in [Alderson et al. (2023)](https://www.nature.com/articles/s41586-022-05591-3), and consist of the transmission spectrum of the exoplanet in the 3-5 $\\mu$m range. This is a couple-of-hundred integrations, 70-group dataset.\n",
    "\n",
    "To get started, let's load some libraries that we will use in the following notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ccca95-97c0-4a4c-88e6-7c8d5d989ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to set these enviromental variables for this notebook to work properly:\n",
    "%set_env CRDS_PATH $HOME/crds_cache\n",
    "%set_env CRDS_SERVER_URL https://jwst-crds.stsci.edu\n",
    "\n",
    "# Import standard python libraries:\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('ticks')\n",
    "\n",
    "# Import a 2D median filter:\n",
    "from scipy.signal import medfilt2d\n",
    "\n",
    "# Import Observations from astroquery.mast to perform data downloads:\n",
    "from astroquery.mast import Observations\n",
    "\n",
    "# Import some JWST Calibration pipieline libraries we will use to load and calibrate our data:\n",
    "import jwst\n",
    "from jwst import datamodels\n",
    "from jwst.pipeline import calwebb_detector1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2950b6-e266-495e-ab56-3957dca97106",
   "metadata": {},
   "source": [
    "Check pipeline version we will be using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72881254-9c29-4cc2-9ea8-36716ad5c528",
   "metadata": {},
   "outputs": [],
   "source": [
    "jwst.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a295b7-9f20-4492-93bd-54e4d722398e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Notes on the validity of this notebook</b>: It is important to realize that this notebook, as it is, is likely to be quickly outdated as new algorithms and fixes are implemented into the JWST Calibration pipeline, as well as new methodologies and studies update our knowledge of optimally calibrating JWST data products. An up-to-date list of known JWST pipeline issues (some of which we touch on this notebook) can be found on the <a href=\"https://jwst-docs.stsci.edu/jwst-calibration-pipeline-caveats/known-issues-with-jwst-data-products\">Known Issues with the JWST Data Products</a> JDox page. In doubt, or for any questions, please contact <a href=\"https://jwst-docs.stsci.edu/jwst-help-desk\">the JWST Helpdesk</a>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f3de65",
   "metadata": {},
   "source": [
    "2.<font color='white'>-</font>Downloading & Quick Looks at JWST TSO data <a class=\"anchor\" id=\"download\"></a>\n",
    "------------------\n",
    "\n",
    "The very first step when it comes to analyzing a JWST dataset is to download that data and perform some quick looks so we know the data quality is acceptable to begin with. Here, we will download the `uncal` products, which are one of the \"raw\"-est forms of dataproducts users can download from MAST. We will perform our data download from MAST using `astroquery.mast` and then use the JWST Calibration pipeline to read and have quicklooks at this data. Let's begin!\n",
    "\n",
    "### 2.1<font color='white'>-</font>Downloading TSO data from MAST<a class=\"anchor\" id=\"mast\"></a> ###\n",
    "\n",
    "To download JWST data from MAST, we will use the `Observations` function from the `astroquery.mast` library. To do this, we need to indicate the properties of the dataset of interest. For this we need to figure out what instrument, filter, program ID _and_ target was obseved. Options for JWST TSO instruments are `NIRISS/SOSS`, `NIRSPEC/SLIT`, `NIRCAM/GRISM`, `MIRI/SLITLESS`, `NIRSPEC/SLIT`, etc. Here, we search for `NIRSpec/SLIT`, and the `F290LP;G395H` filter/grating combination, which corresponds to the dataset we want. We define the proposal ID for the ERS program (`1366`) and the name of the target, `WASP-39`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b211a40-1666-43c1-9c26-fe06bd458336",
   "metadata": {},
   "outputs": [],
   "source": [
    "download = False\n",
    "\n",
    "observation = Observations.query_criteria(instrument_name = 'NIRSpec/SLIT', \n",
    "                                          filters='F290LP;G395H', \n",
    "                                          proposal_id = '1366', \n",
    "                                          target_name = 'WASP-39')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f10a82-9cef-4f56-b45d-516ee0a37018",
   "metadata": {},
   "source": [
    "This saves _all_ possible observations in the `observation` variable. Then, we filter all the products to get only the `SCIENCE`, `UNCAL` data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0d45ff-9bf9-41dd-96c2-d62e87f8d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data products:\n",
    "data_products = Observations.get_product_list(observation)\n",
    "\n",
    "# Filter them to get ramps and rateints; only for the fourth segment of data:\n",
    "uncals = Observations.filter_products(data_products, productType = 'SCIENCE', productSubGroupDescription = 'UNCAL')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e21ae4-11d2-43c3-b40b-c6484420ccbe",
   "metadata": {},
   "source": [
    "Let's check these observations out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c3471c-584c-4c03-a6f4-7b947b4b8b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ffcae5-05f1-440f-a43e-965ce0972196",
   "metadata": {},
   "source": [
    "Note how there are 8 data products. The first 2, which have the lowest `size` values are the Target Aquisition exposures, used to lock the data into place. These are _very_ useful to check the quality of the observations (and whether or not they were successful!). The _actual_ TSO data are all the products that follow. Note the latter data are segmented --- this is done in the ground to facilitate the processing of the data. Also note, for the TSO products, there are 3 segments of data per detector (`nrs1` and `nrs2`).\n",
    "\n",
    "Let's download all the data, including the Target Acquisition frames, which might be useful to diagnose the quality of observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeea47a-9e80-405a-8c2d-0474ae915fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if download:\n",
    "    \n",
    "    Observations.download_products(uncals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fe27c6-6d56-451c-94f6-60c374a04969",
   "metadata": {},
   "source": [
    "Great, all downloads are complete! Let's perform some quick looks at those datasets, which we can do right away without the need to actually calibrate our data products.\n",
    "\n",
    "### 2.2<font color='white'>-</font>Quicklook, pt. I: Target Acquisition<a class=\"anchor\" id=\"ta\"></a> ###\n",
    "\n",
    "The first set of data products we will have a look at are the Target Acquisition (TA) frames. These are frames that are used to precisely center objects in JWST, so as to correct from any JWST blind pointing errors. These frames are taken before the optical element that disperses the light for our TSO observations is put into place, and before doing any small slews to the actual science targets (which should be in the worst case scenario a few tens of arcseconds away from the science target).\n",
    "\n",
    "Note there are two TA frames. [This is expected](https://jwst-docs.stsci.edu/jwst-near-infrared-spectrograph/nirspec-operations/nirspec-target-acquisition/nirspec-wide-aperture-target-acquisition); the usual TA WATA procedure (which is used for TSOs) has one exposure that is used to correct for any pointing errors, and a post-correction TA, which is taken as a \"confirmation\" exposure. We can load those frames with the JWST `datamodels`, which as we will see below are extremely useful models to deal with JWST data, as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84335c83-763d-4add-a3e8-e5da91f02cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ta1 = datamodels.RampModel('mastDownload/JWST/jw01366003001_02101_00001-seg001_nrs1/jw01366003001_02101_00001-seg001_nrs1_uncal.fits')\n",
    "ta2 = datamodels.RampModel('mastDownload/JWST/jw01366003001_02101_00002-seg001_nrs1/jw01366003001_02101_00002-seg001_nrs1_uncal.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67afe9c7-80e1-427b-a821-e5e62e13687d",
   "metadata": {},
   "source": [
    "The `data` attribute of those `datamodels` (e.g., `ta1.data`) stores the actual data from those frames. Let's check the dimensions of those first to familiarize ourselves with those data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e77f679-a70c-4ac0-9e27-50e293aa3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Dimensions of the first TA frame: ', ta1.shape)\n",
    "print('Dimensions of the second TA frame: ', ta2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05708b2-87f7-4c0d-bc31-d8ca3d9adda7",
   "metadata": {},
   "source": [
    "The dimensions come in the form `(integrations, groups, pixel, pixel)` --- so both are 1-integration exposures, of 3 groups each, on a 32x32 pixel frame. This actually is exactly what is expected from WATA TA frames!\n",
    "\n",
    "Let's take a look at the last group of the first TA frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ea799f-a6c1-471d-ab8b-1b5428bd21d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "im = plt.imshow(ta1.data[0, -1, :, :], aspect = 'auto', origin = 'lower', interpolation = None)\n",
    "\n",
    "plt.plot([0,31], [31./2., 31./2.], '--', color = 'white')\n",
    "plt.plot([31./2., 31./2.], [0,31], '--', color = 'white')\n",
    "\n",
    "im.set_clim(3000,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbae5a66-6fb4-4626-8acd-8e4e86aead7b",
   "metadata": {},
   "source": [
    "Nice! There is a source, although it appears slightly off-center, it is reasonably placed around the center of the frame. Let's take a look at the second one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a6d6b-7efc-402a-b5f0-38f97fad7c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "\n",
    "im = plt.imshow(ta2.data[0, -1, :, :], aspect = 'auto', origin = 'lower', interpolation = None)\n",
    "\n",
    "plt.plot([0,31], [31./2., 31./2.], '--', color = 'white')\n",
    "plt.plot([31./2., 31./2.], [0,31], '--', color = 'white')\n",
    "\n",
    "im.set_clim(3000,10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652c8ddd-5ccf-465d-84df-dba9de002c2b",
   "metadata": {},
   "source": [
    "Same story. It seems, overall, the TA was fairly successful. Let's move to check the science data next!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee3cfe5-009e-4088-a00b-ddf7b0cb212e",
   "metadata": {},
   "source": [
    "### 2.3<font color='white'>-</font>Quicklook, pt. II: `datamodels` & TSO Science Data Products<a class=\"anchor\" id=\"science\"></a> ###\n",
    "\n",
    "Next up, we will load the **TSO science** data products so we can interact with them. Once again, we open them through the JWST `datamodels` --- and store all segments of data on lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035daa26-b70a-46da-a8fc-3dd63cb41602",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1 = [datamodels.RampModel('mastDownload/JWST/jw01366003001_04101_00001-seg001_nrs1/jw01366003001_04101_00001-seg001_nrs1_uncal.fits'),\n",
    "              datamodels.RampModel('mastDownload/JWST/jw01366003001_04101_00001-seg002_nrs1/jw01366003001_04101_00001-seg002_nrs1_uncal.fits'),\n",
    "              datamodels.RampModel('mastDownload/JWST/jw01366003001_04101_00001-seg003_nrs1/jw01366003001_04101_00001-seg003_nrs1_uncal.fits')]\n",
    "\n",
    "uncal_nrs2 = [datamodels.RampModel('mastDownload/JWST/jw01366003001_04101_00001-seg001_nrs2/jw01366003001_04101_00001-seg001_nrs2_uncal.fits'),\n",
    "              datamodels.RampModel('mastDownload/JWST/jw01366003001_04101_00001-seg002_nrs2/jw01366003001_04101_00001-seg002_nrs2_uncal.fits'),\n",
    "              datamodels.RampModel('mastDownload/JWST/jw01366003001_04101_00001-seg003_nrs2/jw01366003001_04101_00001-seg003_nrs2_uncal.fits')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c91da3-d4bf-4e94-a176-6743bcfde91a",
   "metadata": {},
   "source": [
    "Note how we load each segment of data for each detector in simple python `list`s! This is the simplicity that these `datamodels` offer. We explore them a bit more below before continuing to the next Section.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Note on memory usage</b>: Loading data products in lists is very useful, but be aware that in particular for TSOs --- which typically involve large data volumes --- Random-Access Memory (RAM) might be severely impacted. The above loaded data products, for instance, take of order ~8 GB --- and this will only be larger as we run pipeline steps below, which convert data products from, e.g., <code>int</code>s to <code>float</code>s, taking even more space. For a typical TSO, when running the pipeline steps we'll run below, consider on the order of ~50 GB will be used.</div>\n",
    "\n",
    "JWST `datamodels` are very handy. First of all, extracting information such as instrument/mode, dates of observation, and basically any header value one would want to extract from those `fits` files is a breeze --- **even if one doesn't know the exact header keyword to look for**. To showcase the usefulness of these `datamodels`, suppose we wanted to check the date of those observations. One might not know the exact keyword that stores this information, but one can use the handy `search` function of `datamodels` to figure this out. We do this for the NRS1 detector data stored above as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76809c-e8d7-40a2-b206-d67613ff8ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].search('date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a034dd-e93e-41e4-a707-cc9e14762e0a",
   "metadata": {},
   "source": [
    "Note how we just added a word similar to what we were looking for, and then this function will take a look to find where similar words are located in the `datamodels` attribute tree. From the above, it seems this information is in `meta.date`. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3f0b00-4589-4a15-adb8-5a956e1b23c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].meta.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4eb9b-d3b7-4ae2-8757-c26dbc4a8dd2",
   "metadata": {},
   "source": [
    "It works! Note `date_beg` is the one we would be typically interested in checking (which was when the observartions happened). Let's try another one. Suppose we wanted to know the name of the PI of this program. Again, the key word here is `pi` so let's insert that one in the `search` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bf6499-4621-4ad8-83c9-1f5e53cc110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].search('pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e8c1d5-d797-443c-a126-8b1a8ba24a9d",
   "metadata": {},
   "source": [
    "So this exists under `meta.program.pi_name`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6d6668-f2fe-4b4a-a5de-a8d4c82a4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].meta.program.pi_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093934a7-f0d3-497f-a2c5-00f5e19c386d",
   "metadata": {},
   "source": [
    "Very handy! Let's now take a quick look at the science data. As seen above for the [TA frame analysis](#ta), this data lives in the `data` attribute. Let's see how this looks like for the first segment of the NRS1 data by exploring the dimensions of the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa4c4f4-c8ee-437d-a5e8-115cb32eb670",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21230db-23d8-40cc-8cac-9dcd1c8bf041",
   "metadata": {},
   "source": [
    "As for the TA frame, dimensions come in the form `(integrations, groups, pixel, pixel)`. So this is a 155-integration segment, with 70 groups per integration each, on a subarray of dimensions 32 x 2048 --- that sounds about right for NIRSpec/G395H exposures. Let's take a look at integration number 10, last group, of the NRS1 detector to see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5614537d-88e5-4a82-8a2c-535979b1d4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.title('Uncal NRS1 data; first segment, integration 10, last group')\n",
    "im = plt.imshow(uncal_nrs1[0].data[10, -1, :, :], aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(3000,9000)\n",
    "\n",
    "plt.colorbar(label = 'Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d462d3-5cc4-46c0-8b45-19fd60013336",
   "metadata": {},
   "source": [
    "What about the NRS2 data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6a9703-ad52-41dd-9a9a-ec3894516008",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.title('Uncal NRS2 data; first segment, integration 10, last group')\n",
    "im = plt.imshow(uncal_nrs2[0].data[10, -1, :, :], aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(6000,12000)\n",
    "\n",
    "plt.colorbar(label = 'Counts')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6118f3-2ad6-4ab0-a45c-382a6de63c41",
   "metadata": {},
   "source": [
    "The data looks great! The central, sword-like feature is the actual spectrum of WASP-39. NRS1 holds data from 3-4 microns, NRS2 from 4-5 microns. All the background structure one sees here, however, is mostly dominated by detector-level effects --- we deal with those in the next Section of this Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49194a6",
   "metadata": {},
   "source": [
    "3.<font color='white'>-</font>A TSO tour through the `Detector1` stage <a class=\"anchor\" id=\"setup\"></a>\n",
    "------------------\n",
    "\n",
    "The `uncal` data products we loaded above contain a series of detector systematic effects that we need to remove before our data is ready for science. Now, we will move to calibrating those TSO `uncal` data products, which will take care of most of those effects. \n",
    "\n",
    "To perform this calibration, here we will follow most of the steps outlined in the `calwebb_detector1` or \"Stage 1\" processing described in the [JWST Calibration pipeline documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/pipeline/calwebb_detector1.html) --- in particular, the one defined for \"Near-IR\" instruments, such as NIRSpec. This Stage 1 processing for Near-IR TSOs is defined by a series of steps, which in order are:\n",
    "\n",
    "1. `group_scale` (not relevant for our datasets)\n",
    "2. `dq_init`\n",
    "3. `saturation`\n",
    "4. `superbias`\n",
    "5. `refpix`\n",
    "6. `linearity`\n",
    "7. `dark_current`\n",
    "8. `jump`\n",
    "9. `ramp_fitting`\n",
    "10. `gain_scale` (not relevant for our datasets)\n",
    "\n",
    "We will slightly modify and/or add some steps to suit our TSO needs below --- let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be76375",
   "metadata": {},
   "source": [
    "### 3.1<font color='white'>-</font>Checking data quality flags <a class=\"anchor\" id=\"dqflags\"></a>\n",
    "\n",
    "An important component of any TSO analysis is to flag bad pixels, pixels identified as cosmic rays and/or identify saturated pixels. Bad pixels are, in fact, curated by the instrument teams in what we colloquially refer to as a \"bad pixel mask\" --- a mask one can \"attach\" to the data products with the JWST Calibration pipeline. This is exactly what the first step in the pipeline, the `dq_init` step, does. \n",
    "\n",
    "#### 3.1.1 Running & understanding the `dq_init` step\n",
    "\n",
    "Let's run the `dq_init` step on the first segment of our data products for NRS1 and NRS2: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a184ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's run the DQ init step; first for the first segment of the NRS1 detector:\n",
    "print('Running dq_init on NRS1:')\n",
    "nrs1_seg1_dqinit = calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nrs1[0])\n",
    "\n",
    "# Now, same on the NRS2 detector:\n",
    "print('Running dq_init on NRS2:')\n",
    "nrs2_seg1_dqinit = calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nrs2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a7c514-6606-4c2d-89eb-e5c60ca96186",
   "metadata": {},
   "source": [
    "All right, data-quality flags have been attached to our uncalibrated data products. To figure out why these are so useful, let's take a look at this bad pixel mask that was attached to our data products; in particular, let's peek at the one attached to the NRS1 detector products. This mask lives in the `pixeldq` attribute of our products (e.g., `nrs1_seg1_dqinit.pixeldq`). To familiarize ourselves with this, let's print the dimensions of this mask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37733464-8f27-4dad-bb7a-3f4c374338ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrs1_seg1_dqinit.pixeldq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c75f617-64df-4121-a450-fd2ac27a4a69",
   "metadata": {},
   "source": [
    "As expected, it has the same size as our subarray data. [As per the documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html?highlight=data%20quality%20flags#data-quality-flags), most data-quality (DQ) flags should be zero in the subarray; let's plot this array to see how many of them get away from this value and where:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da21cbde-850a-4ad5-9d9a-567a82b0d84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.title('Non-zero data-quality values across the subarray')\n",
    "im = plt.imshow(nrs1_seg1_dqinit.pixeldq, aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(-0.5,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7cbb8f-3281-4bb5-aee2-d1d85bed82e4",
   "metadata": {},
   "source": [
    "All right --- so there are \"special\" pixels all over the place! But, what are the `pixeldq` values telling us? Let's print the pixel in the very corner of the subarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c5e735-e12d-4cef-ba9a-e93fbb4b9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrs1_seg1_dqinit.pixeldq[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7b1487-041d-4e6c-b573-97f77f99c525",
   "metadata": {},
   "source": [
    "[According to the documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html?highlight=data%20quality%20flags#data-quality-flags), this pixel is a **reference pixel**. This makes sense: for NRS1 and NRS2 in this NIRSpec/G395H mode, the 4 pixel columns on the left-most end and the 4 pixel columns on the right-most end are indeed, reference pixels.\n",
    "\n",
    "#### 3.1.2 Dynamically translating data-quality flags to human-readable form\n",
    "\n",
    "Looking back and forth from the documentation page the data-quality flag values we read from our data-products is a very tedious task. In addition, as we will see below, a pixel can have eventually several flags (e.g., saturated, has a cosmic-ray, etc.) which will, in turn, change some of its data-quality flags to account for this. \n",
    "\n",
    "A handy function to convert those data-quality flag numbers to \"human-readable\" form is actually inside the `datamodels` class --- the `datamodels.dqflags`. This simply takes in a data-quality value, and spits out a `set` with strings defining what this is telling us given a so-called \"mnemonic map\" --- one which is actually already loaded in the `datamodels.dqflags.pixel` dictionary.\n",
    "\n",
    "Let's try it out on the data-quality value we observed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc893c6d-d4f9-4a3e-af44-7895ba4c207a",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodels.dqflags.dqflags_to_mnemonics(2147483648, mnemonic_map = datamodels.dqflags.pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e09913-917e-4be6-ac14-384a0545696a",
   "metadata": {},
   "source": [
    "Indeed, we get back what we knew --- that is a reference pixel! With this handy-dandy function, we can write a simple snippet to figure out the total tally of all bad pixels as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859e0bc0-b336-4a62-b608-6ac934eb0cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary that will save all types of bad pixels:\n",
    "bad_pixels = {}\n",
    "\n",
    "rows, columns = nrs1_seg1_dqinit.pixeldq.shape\n",
    "\n",
    "# Iterate through every row and column:\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(nrs1_seg1_dqinit.pixeldq[row, column], \n",
    "                                                     mnemonic_map = datamodels.dqflags.pixel)\n",
    "\n",
    "        # Iterate through the possible flags (it can be more than one!):\n",
    "        for bp in bps:\n",
    "\n",
    "            # If already in the bad_pixels dict, simply add 1 to the counter. If not, create and instantiate to one:\n",
    "            if bp in bad_pixels.keys():\n",
    "\n",
    "                bad_pixels[bp] += 1\n",
    "\n",
    "            else:\n",
    "\n",
    "                bad_pixels[bp] = 1\n",
    "\n",
    "# Now print total tally:\n",
    "total_pixels = rows * columns\n",
    "\n",
    "print('From a total of ',total_pixels,' pixels, the \"bad-pixel\" tally is as follows:\\n')\n",
    "for bp in list( bad_pixels.keys() ):\n",
    "\n",
    "    print('-> ',bad_pixels[bp], \n",
    "          'pixels marked as '+bp, \n",
    "          '({0:.2f}% of pixels)'.format(100*(bad_pixels[bp]/float(total_pixels))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e23c07a",
   "metadata": {},
   "source": [
    "Based on our discussion above, we can see some numbers above make sense. For instance, 256 `REFERENCE_PIXELS` make sense as there are a total of 8 columns (4 to the left, 4 to the right of the subarray) with reference pixels --- given the subarray height is 32 pixels, this gives a total of 256 reference pixels as expected.\n",
    "\n",
    "Let's go ahead now and attach this bad pixel mask to all the segments of data, for both NRS1 and NRS2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe5f40-e7f5-4d51-ab91-5b5c4130186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "for i in range(nsegments):\n",
    "\n",
    "    uncal_nrs1[i] = calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nrs1[i])\n",
    "    uncal_nrs2[i] = calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nrs2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5516bd-43b5-4e0a-8223-3a4a31d633ff",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note on saving data products with the JWST Calibration Pipeline</b>: Sometimes, one might find it useful to save data products after running each step into <code>.fits</code> files, so we can have \"intermediate steps\" stored in our system that we can check at a later time. This can be done when running any of the steps by adding the <code>save_results = True</code> flag to the step calls, e.g., <code>calwebb_detector1.dq_init_step.DQInitStep.call(uncal_nrs1[i], save_results = True)</code>. An output directory can also be defined by using the <code>output_dir</code> parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd854cc-47b5-457d-858a-08264af5a9c7",
   "metadata": {},
   "source": [
    "### 3.2<font color='white'>-</font>Identifying saturated pixels <a class=\"anchor\" id=\"saturation\"></a>\n",
    "\n",
    "\n",
    "One very important detail in JWST data analysis involves checking which pixels are \"saturated\" or not. Saturation in the JWST context is an [instrument-by-instrument defined upper signal level](https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-tso-saturation), typically defined as a limit above which detector effects not modeled by the pipeline might start to kick-in. As such, identifying which pixels are above this limit is important. In particular, the pipeline tends to omit those pixels from most analyses, as they might introduce unwanted systematic effects.\n",
    "\n",
    "#### 3.2.1 Running and understanding the `saturation` step\n",
    "\n",
    "Through the analysis of calibration datasets, the JWST instrument teams have defined signal values for each pixel above which they are considered as \"saturated\". This identification is done through the `saturation` step --- the next step of the JWST pipeline for Detector 1. Let's run it for the very first segment of data for NRS1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00464d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run saturation step:\n",
    "saturation_results = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nrs1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75bfef9",
   "metadata": {},
   "source": [
    "The saturation step works by primarily comparing the observed count values with the saturation signal-levels defined for each pixel in a reference file. As can be seen above, that reference file is indicated by the line `stpipe.SaturationStep - INFO - Using SATURATION reference file [yourfile]`. In the case of our run at the time of writing, this was the `jwst_nirspec_saturation_0028.fits` file --- but this might change as new analyses are made and the reference files get updated. \n",
    "\n",
    "In addition, at the time of writing, the `saturation` step in the JWST Calibration pipeline [by default flags not only pixels that exceed the signal limit defined by the instrument teams but also all `n_pix_grow_sat` pixels around it](https://jwst-docs.stsci.edu/jwst-science-calibration-pipeline-overview/jwst-operations-pipeline-build-information/jwst-operations-pipeline-build-8-0-release-notes#JWSTOperationsPipelineBuild8.0ReleaseNotes-charge_spilling); which at the time of writing is set to a default of `1`. That means that if a given pixel exceeds the signal limit, all 8 pixels around it will be marked as saturated as well. This is done because it has been observed that \"charge spilling\" can be an issue --- i.e., charge going from one pixel to another. While such migration of charge happens at a wide range of count levels, this is particularly dramatic when a pixel saturates --- reason by which this is set in the pipeline.\n",
    "\n",
    "We can check which pixels are saturated in a similar way as to how we checked the data-quality flags in [Section 3.1](#dqflags). The only difference with that analysis is that saturated pixels are integration and group-dependant, i.e., a property of a given pixel _in a given integration and group_. In other words, a pixel that is saturated in one integration and group might have \"recovered\" by the next integration and group.\n",
    "\n",
    "To figure out the data-quality for all integrations and all groups we look at the `groupdq` attribute of our data products instead of the `pixeldq` which we used above. To familiarize ourselves with this, let's print the dimensions of this array first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab2ca2a-139f-4511-b354-d4ec9c66f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_results.groupdq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cca152-fdd6-4f57-97df-a770da5eb613",
   "metadata": {},
   "source": [
    "As expected, it has dimensions `(integrations, groups, row pixels, column pixels)`, just like the `data` array. The flags in the `groupdq` array follow the same structure as [all the data-quality flags described in the documentation](https://jwst-pipeline.readthedocs.io/en/latest/jwst/references_general/references_general.html?highlight=data%20quality%20flags#data-quality-flags). \n",
    "\n",
    "#### 3.2.2 Exploring saturated pixels via the `groupdq` array\n",
    "\n",
    "To illustrate how to use the `groupdq`, let's pick the last group of integration 10 again and see if any pixels seem to be saturated --- we also count all of the saturated pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17085a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every row and column of integration number 10, last group:\n",
    "integration, group = 10, -1\n",
    "nsaturated = 0\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(saturation_results.groupdq[integration, group, row, column], \n",
    "                                                      mnemonic_map = datamodels.dqflags.pixel)\n",
    "        \n",
    "        # Check if pixel is saturated; if it is...\n",
    "        if 'SATURATED' in bps:\n",
    "\n",
    "            # ...print which pixel it is, and...\n",
    "            print('Pixel ({0:},{1:}) is saturated in integration 10, last group'.format(row, column))\n",
    "\n",
    "            # ...count it:\n",
    "            nsaturated += 1\n",
    "\n",
    "print('\\nA total of {0:} out of {1:} pixels ({2:.2f}%) are saturated'.format(nsaturated, \n",
    "                                                                             rows*columns, \n",
    "                                                                             100 * nsaturated / float(rows * columns)\n",
    "                                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0a4728",
   "metadata": {},
   "source": [
    "As can be seen, not many pixels are saturated on a given group. Let's see how the up-the-ramp samples look like for one of those pixels --- let's say, pixel `(3, 100)`. Let's show in the same plot the group data-quality flags at each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd84b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row, pixel_column = 3, 100\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.data[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color = 'tomato')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16, color = 'tomato')\n",
    "\n",
    "plt.twinx()\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.groupdq[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color = 'cornflowerblue')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.ylabel('Group Data-quality', fontsize = 16, color = 'cornflowerblue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c506c-e069-4f37-89da-315b8d1a6231",
   "metadata": {},
   "source": [
    "Very interesting plot! Note that all groups appear to be saturated after group ~13. Likely a cosmic-ray hit happened at this group which left the pixel at a very high count number from group 14 up to the end of the ramp."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d8613",
   "metadata": {},
   "source": [
    "#### 3.2.3 Setting custom saturation limits with the `saturation` reference file\n",
    "\n",
    "TSOs often obtain data from bright stars that might quickly (i.e., first few groups) give rise to saturated pixels. As described in some early JWST results (see, e.g., [Rustamkulov et al., 2023](https://www.nature.com/articles/s41586-022-05677-y)), in some cases one might even want to be a bit more aggressive on the level of saturation allowed in a given dataset in order to improve on the reliability of the results. As such, understanding how to modify the level of saturation allowed in a given dataset might turn out to be an important skill on real TSO data analysis. \n",
    "\n",
    "The key file that sets the limits used to call a pixel \"saturated\"  is the reference file of the `saturation` step. \n",
    "\n",
    "As discussed above, this can be seen directly on the outputs of the `saturation` step while its running, but it's also saved in our data products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3b1436",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_results.meta.ref_file.saturation.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e277e99",
   "metadata": {},
   "source": [
    "We can actually load this reference file using the `SaturationModel` as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae83fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base directory where reference files are stored (this was defined in the Setup section above):\n",
    "base_ref_files = '$HOME/crds_cache/references/jwst/nirspec/'\n",
    "\n",
    "# Read it in:\n",
    "saturation_ref_file = datamodels.SaturationModel(base_ref_files+saturation_results.meta.ref_file.saturation.name[7:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53a8353-d769-42ec-90eb-002cf429ac24",
   "metadata": {},
   "source": [
    "More often than not, however, the saturation reference file might not match exactly the dimensions of our subarray. This is because the reference file might be padded to match several other subarrays, and thus we have to figure out how to \"cut\" it to match our data. This is, in fact, our case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb58c8a-8645-4f6b-9dc7-497d459c1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_ref_file.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ebe776-8197-47d5-86ed-bcb495c328b1",
   "metadata": {},
   "source": [
    "Luckily, the JWST calibration pipeline has a handy function to transform the dimensions between instruments --- this is the `jwst.lib.reffile_utils.get_subarray_model` function, which recieves an input data model (e.g., the one from our data) along with the reference file, and spits out the same reference file model but with the right dimensions. Let's use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30e8fda-366a-4003-bb78-31b2834e6a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailored_saturation_ref_file = jwst.lib.reffile_utils.get_subarray_model(saturation_results, saturation_ref_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8b6fe1-b134-4037-a9da-16ed439ae789",
   "metadata": {},
   "source": [
    "Indeed, now our \"tailored\" reference file matches our science data dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2de053d-6d1b-4f65-9285-85983c68b6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailored_saturation_ref_file.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40b6402-c7a4-4375-b012-eb8ccce668c6",
   "metadata": {},
   "source": [
    "Let's see how the saturation map looks like for our subarray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "041a6a56-1e8f-41f0-b215-e23b6dd3f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "plt.title('Saturation map for NRS1 (SUB2048 subarray)')\n",
    "im = plt.imshow(tailored_saturation_ref_file.data, aspect = 'auto', origin = 'lower', interpolation = None)\n",
    "\n",
    "plt.colorbar(label = 'Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c9c2eb-cde9-49ab-b73c-d816e129131c",
   "metadata": {},
   "source": [
    "There's clearly some structure, albeit is not exactly clear what values different pixels take. To visualize this, let's print the saturation limit for pixel `(3,100)`, the one we explored above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585feea1-aebc-43d3-80e0-c149c6abcc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "tailored_saturation_ref_file.data[pixel_row, pixel_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c5962-9876-414a-8f07-69775000a6b0",
   "metadata": {},
   "source": [
    "If the counts surpass this limit, the pixel will be considered saturated. To see if this was the case, let's repeat the plot above marking this signal limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df1a0a-24a8-4eeb-8237-29466736d5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row, pixel_column = 3, 100\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.data[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color = 'tomato')\n",
    "\n",
    "plt.plot([1, saturation_results.data.shape[1]+1], \n",
    "         [tailored_saturation_ref_file.data[pixel_row, pixel_column], \n",
    "          tailored_saturation_ref_file.data[pixel_row, pixel_column]],\n",
    "         'r--', \n",
    "         label = 'Signal limit in reference file'\n",
    "        )\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16, color = 'tomato')\n",
    "plt.legend()\n",
    "\n",
    "plt.twinx()\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.groupdq[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color = 'cornflowerblue')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.ylabel('Group Data-quality', fontsize = 16, color = 'cornflowerblue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a911e15-b2ae-499f-85f7-fd0e803a3834",
   "metadata": {},
   "source": [
    "Indeed, this is the case! Note that, as described above, by default for NIRSpec not only this pixel gets marked as saturated, but all pixels around it. To see this, note for instance the same plot as above but for pixel (2,99):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e572279-2d2e-4754-96f3-df4bc1896211",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_row, pixel_column = 2, 99\n",
    "\n",
    "plt.figure(figsize=(7,4))\n",
    "\n",
    "plt.title('Same as above, but for pixel (2,99)')\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.data[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color = 'tomato')\n",
    "\n",
    "plt.plot([1, saturation_results.data.shape[1]+1], \n",
    "         [tailored_saturation_ref_file.data[pixel_row, pixel_column], \n",
    "          tailored_saturation_ref_file.data[pixel_row, pixel_column]],\n",
    "         'r--', \n",
    "         label = 'Signal limit in reference file'\n",
    "        )\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16, color = 'tomato')\n",
    "plt.legend()\n",
    "\n",
    "plt.twinx()\n",
    "\n",
    "plt.plot(np.arange(saturation_results.data.shape[1])+1, \n",
    "         saturation_results.groupdq[integration, :, pixel_row, pixel_column], \n",
    "         'o-', color = 'cornflowerblue')\n",
    "\n",
    "plt.xlim(0.5, saturation_results.data.shape[1]+1.5)\n",
    "plt.ylabel('Group Data-quality', fontsize = 16, color = 'cornflowerblue')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a75f3-3f5a-40c4-82a2-cf66cf6ea439",
   "metadata": {},
   "source": [
    "Note how the signal level has not gone above the limit in the reference file, but it is marked as saturated because pixel (3,100) is. Again, this is to account for possible charge spilling to the pixel.\n",
    "\n",
    "Now, what if we wanted to mark as saturated all pixels, say, larger than 50\\% these saturation values? Well, we can directly modify the reference file and repeat the calculation pointing at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e149dc-27c9-4775-8cf3-6309bb34e6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_ref_file.data = saturation_ref_file.data * 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a625a17-3f8d-4387-87e9-2767783dfc79",
   "metadata": {},
   "source": [
    "To incorporate this new reference file, we simply use the `override_saturation` flag, passing this new `SaturationModel` along: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91279ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run saturation step:\n",
    "saturation_results2 = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nrs1[0], \n",
    "                                                                            override_saturation = saturation_ref_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba32975e",
   "metadata": {},
   "source": [
    "Let's see how many pixels are now counted as saturated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1d409c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through every row and column of integration number 10, last group:\n",
    "integration, group = 10, -1\n",
    "nsaturated = 0\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(saturation_results2.groupdq[integration, group, row, column], \n",
    "                                                      mnemonic_map = datamodels.dqflags.pixel)\n",
    "        \n",
    "        # Check if pixel is saturated; if it is...\n",
    "        if 'SATURATED' in bps:\n",
    "\n",
    "            # ...print which pixel it is, and...\n",
    "            print('Pixel ({0:},{1:}) is saturated in integration 10, last group'.format(row, column))\n",
    "\n",
    "            # ...count it:\n",
    "            nsaturated += 1\n",
    "\n",
    "print('\\nA total of {0:} out of {1:} pixels ({2:.2f}%) are saturated'.format(nsaturated, \n",
    "                                                                             rows*columns, \n",
    "                                                                             100 * nsaturated / float(rows * columns)\n",
    "                                                                            )\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf46ae5",
   "metadata": {},
   "source": [
    "As expected, a much bigger portion! About 5\\% of the pixels in the subarray are now masked (against 0.04\\% from before) as saturated thanks to our higher threshold for flagging.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Note on manually setting the saturation limit</b>: Setting the saturation limit manually should be done with care, and we recommend trying different saturation levels to check whether TSO science is impacted by this choice. In particular, we suggest to <i>never</i> set limits that are above the thresholds defined by the instrument teams, as these are typically set to levels above which the non-linearity correction (see below) is not expected to work.</div>\n",
    "\n",
    "Before moving to the next step, let's run the saturation step on both NRS1 and NRS2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e99ccf3-53d7-4281-b119-c331fcaed288",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "for i in range(nsegments):\n",
    "\n",
    "    uncal_nrs1[i] = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nrs1[i])\n",
    "    uncal_nrs2[i] = calwebb_detector1.saturation_step.SaturationStep.call(uncal_nrs2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0355b1",
   "metadata": {},
   "source": [
    "### 3.3<font color='white'>-</font>Removing detector-level effects: the `superbias` and `refpix` steps <a class=\"anchor\" id=\"refpix\"></a>\n",
    "\n",
    "So far, we have focused on flagging pixels for various effects (e.g., bad pixels, saturation) but we haven't worked directly with the actual counts on our data. In this Section, we deal with various (non-astrophysical) detector-level effects present in our data through two steps in the JWST Calibration pipeline: the `superbias` and the `refpix` steps. \n",
    "\n",
    "#### 3.3.1 Removing the pedestal from the detector: the `superbias` step\n",
    "\n",
    "All detectors have mostly stable, factory-defined pedestal levels, which can be closely monitored with the right calibration exposures. Indeed, instrument teams closely monitor and refine this via what is called the \"super\" bias --- the spatial shape of this pedestal. The JWST Calibration pipeline substracts this pedestal from data via the `superbias` step.\n",
    "\n",
    "Applying this correction to the data is very simple to do; let's apply it once again to the first segment of data for the NRS1 detector, so we can check how our data changes after applying the step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc23ba69",
   "metadata": {},
   "outputs": [],
   "source": [
    "superbias_results = calwebb_detector1.superbias_step.SuperBiasStep.call(uncal_nrs1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3144d5-0b11-4984-9411-cbd4e5e87819",
   "metadata": {},
   "source": [
    "Once again, we can see that there is a particular reference file being used to remove the pedestal, `jwst_nirspec_superbias_0427.fits`, which can be explored in a similar way as how we explored the reference file for the `saturation` step above. Let's see how our data changed after applying this pedestal removal --- let's again take the last group of integration 10 as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079d7caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them:\n",
    "plt.figure(figsize=(10,3))\n",
    "im = plt.imshow(uncal_nrs1[0].data[10,-1,:,:] / np.nanmedian(uncal_nrs1[0].data[10,-1,:,:]), \\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-3,2)\n",
    "plt.title('Before the Superbias step:')\n",
    "\n",
    "plt.colorbar(label = 'Normalized (to median) fluence')\n",
    "\n",
    "# Plot them:\n",
    "plt.figure(figsize=(10,3))\n",
    "im = plt.imshow(superbias_results.data[10,-1,:,:] / np.nanmedian(superbias_results.data[10,-1,:,:]), \\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-3,2)\n",
    "plt.title('After the Superbias step:')\n",
    "plt.colorbar(label = 'Normalized (to median) fluence')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340e93a6",
   "metadata": {},
   "source": [
    "Wow! That's a huge change. Overall, this looks much better. Let's plot the profiles of pixel column index 1500 to have a closer look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85918a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(uncal_nrs1[0].data[10,-1,:,1500], label = 'Before the Superbias step')\n",
    "ax[0].plot(superbias_results.data[10,-1,:,1500], label = 'After the Superbias step')\n",
    "ax[0].set_xlabel('Row pixel index', fontsize = 14)\n",
    "ax[0].set_ylabel('Counts', fontsize = 14)\n",
    "ax[0].set_title('Comparison before/after Superbias step', fontsize = 14)\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].set_title('Same, but median-substracted counts', fontsize = 14)\n",
    "ax[1].plot(uncal_nrs1[0].data[10,-1,:,1500] - np.nanmedian(uncal_nrs1[0].data[0,-1,:,1500]))\n",
    "ax[1].plot(superbias_results.data[10,-1,:,1500] - np.nanmedian(superbias_results.data[0,-1,:,1500]))\n",
    "ax[1].set_xlabel('Row pixel index', fontsize = 14)\n",
    "ax[1].set_ylabel('Counts - Median Counts', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ba157",
   "metadata": {},
   "source": [
    "As can be seen, a ton of structure has been removed. Also, all the pixels seem to be at the same background level. This is a good sign that the Superbias correction has worked, in principle, correctly. \n",
    "\n",
    "However, if we take a more detailed look at background pixels, we can note an interesting pattern. Let's plot a similar cut to the one above, but for column 250 --- which is far away from any illuminted pixels in the detector. Let's also plot the last superbias-corrected group and the second-to-last superbias-corrected group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3a1979-9c92-4dac-ae8c-62fed86fa497",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(superbias_results.data[10,-1,:,250])\n",
    "ax[0].plot([0, 32], [0, 0], 'k--')\n",
    "ax[0].set_xlabel('Row pixel index', fontsize = 14)\n",
    "ax[0].set_ylabel('Counts', fontsize = 14)\n",
    "ax[0].set_title('Superbias-corrected close-up, last group, integration 10', fontsize = 14)\n",
    "ax[0].set_ylim(-250,250)\n",
    "ax[0].set_xlim(0,31)\n",
    "\n",
    "ax[1].set_title('Superbias-corrected close-up, second-to-last group, integration 10', fontsize = 14)\n",
    "ax[1].plot(superbias_results.data[10,-2,:,250])\n",
    "ax[1].plot([0, 32], [0, 0], 'k--')\n",
    "ax[1].set_ylim(-250,250)\n",
    "ax[1].set_xlim(0,31)\n",
    "ax[1].set_xlabel('Row pixel index', fontsize = 14)\n",
    "ax[1].set_ylabel('Counts', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0aeed7-e0f7-4ebb-bc20-6299be052a02",
   "metadata": {},
   "source": [
    "Note how the pedestal correction the `superbias` step has, first of all, **not** brought the background down to **exactly** zero. The answer to this behavior is that there are other, group-dependant detector effects that need to be removed. These are the ones the so-called \"reference pixels\" in the detector aim at correcting for, which is done in the JWST Calibration pipeline via the `refpix` step --- the step we cover next in this Notebook.\n",
    "\n",
    "#### 3.3.2 Removing group-dependant detector effects: the `refpix` step\n",
    "\n",
    "All the JWST detectors contain reference pixels, typically located in some (or all) of the edges of the detectors. These pixels are ones for which their \"sensitivity to light\" has been deactivated, and are thus useful for tracking detector-level effects happening at the time of our observations. While all detectors have those, **not all detector subarrays** contain reference pixels. Some, like in our case, contain reference pixels only in certain portions of the subarray.\n",
    "\n",
    "Let's visualize where those reference pixels are in our subarray by using the `pixeldq` flags:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf15880-a891-4de9-841d-1969b107db6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that will save locations of reference pixels:\n",
    "reference_pixels = np.zeros([rows, columns])\n",
    "\n",
    "# Iterate through every row and column:\n",
    "for row in range(rows):\n",
    "    \n",
    "    for column in range(columns):\n",
    "\n",
    "        # Extract the bad pixel flag(s) for the current pixel at (row, column):\n",
    "        bps = datamodels.dqflags.dqflags_to_mnemonics(superbias_results.pixeldq[row, column], \n",
    "                                                      mnemonic_map = datamodels.dqflags.pixel)\n",
    "\n",
    "        if 'REFERENCE_PIXEL' in bps:\n",
    "\n",
    "            reference_pixels[row, column] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8576ec37-fff3-4e65-aa8d-f03cfc10e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "plt.title('Location of reference pixels in the subarray')\n",
    "im = plt.imshow(reference_pixels, aspect = 'auto', origin = 'lower')\n",
    "\n",
    "# Arrows to indicate edges:\n",
    "plt.text(1800, 22, '4 columns', color = 'white')\n",
    "plt.arrow(1780, 16, 150, -1, width = 5, head_width = 10, head_length = 100, color = 'white')\n",
    "\n",
    "plt.text(100, 22, '4 columns', color = 'white')\n",
    "plt.arrow(268, 16, -150, -1, width = 5, head_width = 10, head_length = 100, color = 'white')\n",
    "\n",
    "im.set_clim(-0.5,0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a183faf7-693b-4ca0-97da-321d9703012a",
   "metadata": {},
   "source": [
    "Note the white edges to the left and the right of the plot above (indicated by arrows)? That's the location of the reference pixels for our subarray. In other words, our subarray has reference pixels to the left and right-most sides, but not on the top or bottom part.\n",
    "\n",
    "This is important to note because when one inspects NIRSpec/G395H data, there are clearly row-by-row effects and column-by-column effects. **The most important row-by-row effect is the so-called \"odd/even\" effect**, whose symptom is that of alternating flux \"jumps\" that is seen in all odd columns at one level, and at even columns at another level. To illustrate this, let's plot all the odd and even rows in our subarray for integration 10, last group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b133a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "for i in range(32):\n",
    "\n",
    "    if i < 2:\n",
    "\n",
    "        if i % 2:\n",
    "\n",
    "            name = 'Even rows'\n",
    "\n",
    "        else:\n",
    "\n",
    "            name = 'Odd rows'\n",
    "\n",
    "    else:\n",
    "\n",
    "        name = None\n",
    "\n",
    "    if i % 2:\n",
    "\n",
    "        plt.plot(superbias_results.data[10, -1, i, :], color = 'tomato', alpha = 0.5, label = name)\n",
    "\n",
    "    else:\n",
    "\n",
    "        plt.plot(superbias_results.data[10, -1, i, :], color = 'cornflowerblue', alpha = 0.5, label = name)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0, 2048)\n",
    "plt.ylim(-50,200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c3e82f",
   "metadata": {},
   "source": [
    "Note how the red and blue curves are all offset from one another due to this \"odd/even\" effect. On the other hand, **the most important \"column-by-column\" detector effect is 1/f noise**. A symptom of this effect is an apparent vertical \"banding\" on the detector, which will become very apparent once we apply the reference pixel correction. **There's a third important detector effect which offsets all pixels up and down throughout the detector on a group-level basis**. As can be seen, the row-by-row and this third, all-pixel offset should be corrected by the reference pixels at the edge of our subarray --- but the 1/f, column-by-column effect should remain.\n",
    "\n",
    "Let's apply the `refpix` step to check how our data looks like after it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad093d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "refpix_results = calwebb_detector1.refpix_step.RefPixStep.call(superbias_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b29e8b-c0c9-4768-ae26-2ed24f92ab66",
   "metadata": {},
   "source": [
    "Let's plot once again the figures above. First, a vertical cut of the profile at pixel column index 250:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d578bc6-4534-440f-ad4f-c648605bb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows = 1, ncols = 2, figsize=(15,5))\n",
    "\n",
    "ax[0].plot(refpix_results.data[10,-1,:,250])\n",
    "ax[0].plot([0, 32], [0, 0], 'k--')\n",
    "ax[0].set_xlabel('Row pixel index', fontsize = 14)\n",
    "ax[0].set_ylabel('Counts', fontsize = 14)\n",
    "ax[0].set_title('Reference pixel-corrected close-up, last group, integration 10', fontsize = 14)\n",
    "ax[0].set_ylim(-250,250)\n",
    "ax[0].set_xlim(0,31)\n",
    "\n",
    "ax[1].set_title('Reference pixel-corrected close-up, second-to-last group, integration 10', fontsize = 14)\n",
    "ax[1].plot(refpix_results.data[10,-2,:,250])\n",
    "ax[1].plot([0, 32], [0, 0], 'k--')\n",
    "ax[1].set_ylim(-250,250)\n",
    "ax[1].set_xlim(0,31)\n",
    "ax[1].set_xlabel('Row pixel index', fontsize = 14)\n",
    "ax[1].set_ylabel('Counts', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6945992-ac4e-453d-a19b-7e26d36864a4",
   "metadata": {},
   "source": [
    "Nice, now flux is down to the zero-level. What about the odd-even effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab7b36-284d-4f69-92e7-1aa7b0c0ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "for i in range(32):\n",
    "\n",
    "    if i < 2:\n",
    "\n",
    "        if i % 2:\n",
    "\n",
    "            name = 'Even rows'\n",
    "\n",
    "        else:\n",
    "\n",
    "            name = 'Odd rows'\n",
    "\n",
    "    else:\n",
    "\n",
    "        name = None\n",
    "\n",
    "    if i % 2:\n",
    "\n",
    "        plt.plot(refpix_results.data[10, -1, i, :], color = 'tomato', alpha = 0.5, label = name)\n",
    "\n",
    "    else:\n",
    "\n",
    "        plt.plot(refpix_results.data[10, -1, i, :], color = 'cornflowerblue', alpha = 0.5, label = name)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlim(0, 2048)\n",
    "plt.ylim(-50,200)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9a9b0d-51ee-4cf9-8426-8cc8e86ea24f",
   "metadata": {},
   "source": [
    "Gone, too!\n",
    "\n",
    "Let's plot the 2D frame of this integration to see how the data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a922c5-0681-451e-affd-02bd2486f2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them:\n",
    "plt.figure(figsize=(10,3))\n",
    "im = plt.imshow(superbias_results.data[10,-1,:,:],\\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-100,100)\n",
    "plt.title('Before the Refpix step:')\n",
    "\n",
    "plt.colorbar(label = 'Counts')\n",
    "\n",
    "# Plot them:\n",
    "plt.figure(figsize=(10,3))\n",
    "im = plt.imshow(refpix_results.data[10,-1,:,:],\\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-100,100)\n",
    "plt.title('After the Refpix step:')\n",
    "plt.colorbar(label = 'Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf297f-1f61-4c05-a6c6-d00909136f3c",
   "metadata": {},
   "source": [
    "That looks better. Note how most of the horizontal structure has been removed.\n",
    "\n",
    "It is interesting to note that the \"banding\" on the columns, as discussed above, has not dissapeared. This is more evident when plotting a series of groups from different integrations; let's plots the groups from integrations 10, 11 and 12:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e32b13-0592-4953-a516-8fefbf7975e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration 10, last group\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title('Integration 10, last group')\n",
    "im = plt.imshow(refpix_results.data[10,-1,:,:],\\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-100,100)\n",
    "plt.colorbar(label = 'Counts')\n",
    "\n",
    "# Integration 10, last group\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title('Integration 11, last group')\n",
    "im = plt.imshow(refpix_results.data[11,-1,:,:],\\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-100,100)\n",
    "plt.colorbar(label = 'Counts')\n",
    "\n",
    "# Integration 12, last group\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.title('Integration 12, last group')\n",
    "im = plt.imshow(refpix_results.data[12,-1,:,:],\\\n",
    "                aspect = 'auto', origin = 'lower', interpolation = 'None')\n",
    "im.set_clim(-100,100)\n",
    "plt.colorbar(label = 'Counts')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3837a984-6b6e-4583-9077-1a5456a480c3",
   "metadata": {},
   "source": [
    "This is, once again, expected as there are no reference pixels in the columns. We will explore how to correct this after going with the `linearity` correction/step, which we discuss next. Before moving on, we apply the superbias and reference pixel step to both detectors, all segments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae102d5-d78c-42c8-bf43-dc54082cbafa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "for i in range(nsegments):\n",
    "\n",
    "    # Apply superbias and refpix to NRS1:\n",
    "    uncal_nrs1[i] = calwebb_detector1.superbias_step.SuperBiasStep.call(uncal_nrs1[i])\n",
    "    uncal_nrs1[i] = calwebb_detector1.refpix_step.RefPixStep.call(uncal_nrs1[i])\n",
    "    \n",
    "    # Apply superbias and refpix to NRS2:\n",
    "    uncal_nrs2[i] = calwebb_detector1.superbias_step.SuperBiasStep.call(uncal_nrs2[i])\n",
    "    uncal_nrs2[i] = calwebb_detector1.refpix_step.RefPixStep.call(uncal_nrs2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b2f7d",
   "metadata": {},
   "source": [
    "### 3.4<font color='white'>-</font>Linearity corrections <a class=\"anchor\" id=\"linearity\"></a>\n",
    "\n",
    "As a pixel accumulates charge, it becomes less and less efficient at generating charge-carriers and/or holding that charge in place. A consequence of this is that the raw -uncalibrated- up-the-ramp samples in JWST detectors are non-linear, with the pixels at lower fluences being almost linear and pixels near the saturation ranges deviating significantly from this behavior. This is the behavior that the `linearity` step in the JWST Calibration pipeline aims to fix.\n",
    "\n",
    "#### 3.4.1 Visualizing and correcting for non-linearity with the `linearity` step\n",
    "\n",
    "To visualize the non-linearity of the up-the-ramp samples, let's take a look at the samples of one of the brightest pixels in our subarray, pixel `(12, 2000)` --- say for integration number 10. Let's plot on top a line fitted to the first 10 pixels, which should be the most \"linear\" of all pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740c49b7-f880-4ce9-8afd-a979ca0230bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngroups = uncal_nrs1[0].data.shape[1]\n",
    "group = np.arange(ngroups) + 1\n",
    "\n",
    "first_groups = 20\n",
    "\n",
    "coeff = np.polyfit(group[:first_groups],  uncal_nrs1[0].data[10, :first_groups, 11, 2000], 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.title('Up-the-ramp sample, integration 10, pixel (11, 2000)')\n",
    "\n",
    "plt.plot( group, uncal_nrs1[0].data[10, :, 11, 2000], 'o-', color = 'black', mfc = 'white', label = 'Up-the-ramp samples')\n",
    "plt.plot( group, np.polyval(coeff, group), 'r--', label = 'Linear fit to first '+str(first_groups)+' groups')\n",
    "\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16)\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 70.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d261731-04de-466c-8f0e-e9dea55af661",
   "metadata": {},
   "source": [
    "Ah --- the ramp is _clearly_ non-linear! Let's apply the `linearity` step to the very first segment of NRS1 to see how well this gets corrected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b099e9a1-4949-4eab-870b-3cb87415aebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linearity step:\n",
    "linearity_results = calwebb_detector1.linearity_step.LinearityStep.call(uncal_nrs1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2882aaf3-0f2d-492b-9a97-07e06ca44d72",
   "metadata": {},
   "source": [
    "Let's try the same plot as above, but with the linearity-corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa85745-3136-4b67-bc39-c727149c459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngroups = uncal_nrs1[0].data.shape[1]\n",
    "group = np.arange(ngroups) + 1\n",
    "\n",
    "first_groups = 20\n",
    "\n",
    "coeff = np.polyfit(group[:first_groups],  linearity_results.data[10, :first_groups, 11, 2000], 1)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.title('Same as above, linearity-corrected')\n",
    "\n",
    "plt.plot( group, linearity_results.data[10, :, 11, 2000], 'o-', color = 'black', mfc = 'white', label = 'Up-the-ramp samples')\n",
    "plt.plot( group, np.polyval(coeff, group), 'r--', label = 'Linear fit to first '+str(first_groups)+' groups')\n",
    "\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16)\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 70.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca8a4da-90ef-4581-a06e-f8a8395e334a",
   "metadata": {},
   "source": [
    "Ah, much better! \n",
    "\n",
    "#### 3.4.2 Testing the accuracy of the `linearity` step\n",
    "\n",
    "It is important to realize that the linearity corrections that the JWST Calibration pipeline applies through the `linearity` step are _not_ perfect. While this is difficult to see with a single integration, this can be studied with multiple integrations --- which helps us beat the noise embedded on single up-the-ramp samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9670f660",
   "metadata": {},
   "source": [
    "One trick to glance at how the linearity of the up-the-ramp samples evolves as one goes up-the-ramp is to note that if the detector is linear, it doesn't matter at which up-the-ramp sample one looks at, the **fluence level should change from group-to-group at _the same rate_ on average**. So one can quickly investigate if linearity is an issue (and if the pipeline is correctly correcting for it) by:\n",
    "\n",
    "1. Taking the difference in fluence between two subsequent groups (say, the last two).\n",
    "2. Taking the difference in fluence between two _other_ subsequent groups (say, the first two).\n",
    "3. Take the ratio between those differences.\n",
    "\n",
    "If the detector is linear, then all the pixels should fall around a ratio of 1. Do they? Let's try this experiment out. Let's first take the difference of the last two and first two groups for all the pixels of all the integrations of the **uncorrected** data --- then take the ratio of those. As we saw above, this should scream \"non-linearity\" all over!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a175321",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_pair = uncal_nrs1[0].data[:, -1, :, :] - uncal_nrs1[0].data[:, -2, :, :]\n",
    "first_pair = uncal_nrs1[0].data[:, 1, :, :] - uncal_nrs1[0].data[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a060399",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = last_pair / first_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596ec70a",
   "metadata": {},
   "source": [
    "Let's now flatten those arrays and plot them as a function of total fluence at the very last group. If linearity weren't an issue, all of these should line around 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d14c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_ratio = ratio.flatten()\n",
    "flattened_fluences = uncal_nrs1[0].data[:, -1, :, :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53df40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(flattened_fluences, flattened_ratio, '.', alpha = 0.01, color = 'black')\n",
    "plt.plot([0,35000], [1., 1.], 'r--')\n",
    "plt.ylim(0.5,1.5)\n",
    "plt.xlim(0,35000)\n",
    "plt.xlabel('Fluence at the last group (counts)', fontsize = 14)\n",
    "plt.ylabel('(Last / First) Group differences', fontsize = 14)\n",
    "plt.title('No linearity correction', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbcd297",
   "metadata": {},
   "source": [
    "Indeed, the data does _not_ line up around 1. So linearity _is_ an issue (as we already observed in the up-the ramp samples before)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47147cb6",
   "metadata": {},
   "source": [
    "All right, let's try the same experiment but now on the linearity-corrected data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8002de",
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_last_pair = linearity_results.data[:, -1, :, :] - linearity_results.data[:, -2, :, :]\n",
    "corrected_first_pair = linearity_results.data[:, 1, :, :] - linearity_results.data[:, 0, :, :]\n",
    "corrected_ratio = corrected_last_pair / corrected_first_pair"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3041b82e",
   "metadata": {},
   "source": [
    "Let's plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08942e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_corrected_ratio = corrected_ratio.flatten()\n",
    "flattened_corrected_fluences = linearity_results.data[:, -1, :, :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f30a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6,4))\n",
    "plt.plot(flattened_corrected_fluences, flattened_corrected_ratio, '.', alpha = 0.005, color = 'black')\n",
    "plt.plot([0,35000], [1., 1.], 'r--')\n",
    "plt.ylim(0.5,1.5)\n",
    "plt.xlim(0,35000)\n",
    "plt.xlabel('Fluence at the last group (counts)', fontsize = 14)\n",
    "plt.ylabel('(Last / First) Group differences', fontsize = 14)\n",
    "plt.title('After linearity correction', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1896c9fd-da47-4def-9882-62cde592a8bb",
   "metadata": {},
   "source": [
    "That looks **much** better. Note, however, that as discussed above the corrections are *not* perfect. In particular, below about 20,000 counts it seems the correction makes the last group difference to be slightly larger than the first group differences; this changes for the larger fluences, where the last group difference seems to have a _smaller_ flux than the first group differences. This is actually consistent with a _charge migration_ hypothesis, on which pixels that receive larger fluences _lose_ charge to neighboring pixels that receive them. Testing this hypothesis is, of course, outside of the present Notebook --- but this showcases that plots like the ones above are fundamental to make sense of data and the overall accuracy and precision of non-linearity corrections.\n",
    "\n",
    "Before moving to the next step, we apply the `linearity` step to all our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95974a7f-25d9-41fe-8a3d-b02cb8a58ccf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "for i in range(nsegments):\n",
    "\n",
    "    # Apply the linearity step to NRS1:\n",
    "    uncal_nrs1[i] = calwebb_detector1.linearity_step.LinearityStep.call(uncal_nrs1[i])\n",
    "    \n",
    "    # Same, for NRS2:\n",
    "    uncal_nrs2[i] = calwebb_detector1.linearity_step.LinearityStep.call(uncal_nrs2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91161c62",
   "metadata": {},
   "source": [
    "### 3.5<font color='white'>-</font>Removing the dark current <a class=\"anchor\" id=\"darkcurrent\"></a>\n",
    "\n",
    "One of the last steps before the most computationally expensive steps in the pipeline is the `dark_current` step. This step grabs a reference file that calculates the dark current at each group, and applies the same correction to every integration in the same way. \n",
    "\n",
    "It is unclear if this step is helpful at all for TSOs, where signals are typically high (and thus, the dark current is but a very small addition to the total current gathered in a TSO), but we go ahead and apply this step nonetheless in our data. First, to check what changes this step does in our data, we apply it on the first NRS1 segment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41eade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the darkcurrent step:\n",
    "darkcurrent_results = calwebb_detector1.dark_current_step.DarkCurrentStep.call(uncal_nrs1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755efd9f",
   "metadata": {},
   "source": [
    "Let's see its impact on products before the dark current correction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e46eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot them:\n",
    "plt.figure(figsize=(10,3))\n",
    "im = plt.imshow(uncal_nrs1[0].data[10,-1,:,:] / np.nanmedian(uncal_nrs1[0].data[10,-1,:,:]), \\\n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(-3,2)\n",
    "plt.colorbar(label = 'Normalized (to median) fluence')\n",
    "plt.title('Before the DarkCurrent step:')\n",
    "\n",
    "# Plot them:\n",
    "plt.figure(figsize=(10,3))\n",
    "im = plt.imshow(darkcurrent_results.data[10,-1,:,:] / np.nanmedian(darkcurrent_results.data[10,-1,:,:]), \\\n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "\n",
    "im.set_clim(-3,2)\n",
    "plt.title('After the DarkCurrent step:')\n",
    "plt.colorbar(label = 'Normalized (to median) fluence')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342f349d",
   "metadata": {},
   "source": [
    "Difficult to see the impact from those simple plots. \n",
    "\n",
    "Let's quantify \"how much\" dark current there is by simply calculating the average (accross integrations) percentage of dark current on the last group. The reason for doing this in the last group is that this is the group that accumulates _the most_ dark current --- as dark current grows as a function of the number of groups. \n",
    "\n",
    "To do this, we consider that for a non-dark current corrected signal $S_{DC}$, if we substract the dark-current corrected signal $S_{DC, corrected}$ we get the dark current signal back, i.e., $S_{DC} - S_{DC, corrected} = DC$; dividing this by the dark-current corrected signal gives us the percentage of dark current signal on each pixel. Let's calculate a map of this for all integrations and take the median of those:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dc5d9d-e417-4fc3-a483-4b0fe89e3265",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "\n",
    "# Calculate the (average) percent change of the signal accross all integrations --- this is (Dark Signal) / (\"Real\" signal):\n",
    "percent = ((linearity_results.data[:,-1,:,:] - darkcurrent_results.data[:,-1,:,:]) / \n",
    "           darkcurrent_results.data[:,-1,:,:]) * 100\n",
    "\n",
    "percent = np.nanmedian(percent, axis = 0)\n",
    "\n",
    "# Plot --- minimum and maximum are bounded to about 20:\n",
    "im = plt.imshow(percent, \\\n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(0, 25)\n",
    "plt.colorbar(label = '% of Dark Signal')\n",
    "plt.title('Median impact of dark signal on the last group')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab230130-64be-4014-9ec9-7e9e92e04622",
   "metadata": {},
   "source": [
    "All right, so there _is_ an impact on the order of ~10-15% for the last group, at least on the left-hand side of the detector where there is not a lot of signal --- i.e., left-most of pixel column 500. Right-most of this, it seems the impact is very low, of order ~5\\% in background pixels (i.e., close to the upper and lower edges) and even lower in the location of the spectra itself --- less than ~0.03\\% at the peak signal level.\n",
    "\n",
    "How much the above impacts a given TSO must be defined on a target-by-target basis. In the worst-case scenarios, this impact might not be purely aesthetical --- this dark current can give rise to transit depth dilutions in transiting exoplanet science, for instance; just as any non-accounted background signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a326d2-f0e6-46ff-88ad-fe70b2d126bf",
   "metadata": {},
   "source": [
    "In the case of this notebook, we apply it nonetheless to all the detector-level data of both NRS1 and NRS2 in all segments, but we leave as an excercise to the reader to perform a full re-reduction with and without dark-current to see the impact of this step on this particular dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3d1694-ccfe-478e-a7b4-a8861ad18028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "for i in range(nsegments):\n",
    "\n",
    "    # Apply the dark_current step to NRS1:\n",
    "    uncal_nrs1[i] = calwebb_detector1.dark_current_step.DarkCurrentStep.call(uncal_nrs1[i])\n",
    "    \n",
    "    # Same, for NRS2:\n",
    "    uncal_nrs2[i] = calwebb_detector1.dark_current_step.DarkCurrentStep.call(uncal_nrs2[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7826331d-c269-4c96-a124-f7d9b61ad742",
   "metadata": {},
   "source": [
    "\n",
    "### 3.6<font color='white'>-</font>Handling 1/f noise at the group-level<a class=\"anchor\" id=\"1overf\"></a>\n",
    "\n",
    "The `dark_current` step is the last in the near-infrared detectors that tries to remove any detector-level effects at the group-level, at least in the current version of the pipeline. However, some detector effects still remain --- in particular, the \"infamous\" [1/f noise](https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-noise-sources). This is a type of noise that appears on every group independently, but it _can_ be buried below some cosmetic effects if we were to plot the groups after each other --- let's showcase this by plotting the first 5 groups of the 10th integration of the NRS1 detector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9294bff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    plt.figure(figsize=(12,3))\n",
    "    im = plt.imshow(uncal_nrs1[0].data[10,i,:,:], interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    im.set_clim(-200,200)\n",
    "    plt.title('10th integration, group '+str(i+1))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce980b5c-d383-4d50-88b4-2167d0b47ca3",
   "metadata": {},
   "source": [
    "By eye, the groups appear to be more or less the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94338c17",
   "metadata": {},
   "source": [
    "Instead, let's plot the differences between groups --- this should remove signals that are common to each group, and only leave any residual noise in the background region. Let's plot the differences among the first 5 groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1cfcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    plt.figure(figsize=(12,3))\n",
    "    \n",
    "    im = plt.imshow(uncal_nrs1[0].data[10,i+1,:,:] - uncal_nrs1[0].data[10,i,:,:], \n",
    "                    interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "    im.set_clim(-25,25)\n",
    "    plt.title('10th integration, group '+str(i + 2)+' - group '+str(i+1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24430360",
   "metadata": {},
   "source": [
    "See those vertical stripes changing location from difference to difference? **That's a symptom of the so-called [\"1/f\" noise](https://jwst-docs.stsci.edu/methods-and-roadmaps/jwst-time-series-observations/jwst-time-series-observations-noise-sources)**. For a detailed description of this type of noise as seen in JWST infrared detectors, a good reference is [Schlawin et al., 2020](https://arxiv.org/abs/2010.03564). In short, the vertical stripes that are observed are, in fact, a product of a time-series that occurs _accross_ the pixels in each group (really, in each frame --- but in the case of the readout mode being used here, a frame _is_ a group). \n",
    "\n",
    "When pixels are read in a given up-the-ramp sample, the detector electronics read each pixel's signal sequentially. It starts with the pixel in one of the corners, and then moves to the next pixel in the same column taking, in the case of this detector (and most currently supported modes for JWST detectors), 10 microseconds. Then, it jumps to the next in the same column in 10 microseconds, and so on. When it reaches the end of a column after jumping in this case through 32 pixels, the electronics are ordered to wait 120 microseconds exactly before moving to the next column. And then the process repeats. 1/f noise in this context arises because the readout electronics have noise associated with it, and the power spectral density (PSD) of this noise, if one \"tags\" each pixel with the clocking process described above, has a $1/f^{\\beta}$ shape --- with the power index, $\\beta$, being very close to 1. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\"> <b>Note on 1/f as a noise source</b>: It is important to note that 1/f noise is a particularly important noise source to pay special attention in near-infrared detectors such as NIRSpec, NIRISS and NIRCam, but is not such a big piece of the puzzle in MIRI IR detectors. As such, while the discussion here could be relevant and applicable to NIR detectors, it might not be for MIRI detectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae2159f-41f9-4117-9e6b-a10afb7ef0b0",
   "metadata": {},
   "source": [
    "There's a wide array of techniques and software [from the community](https://www.stsci.edu/jwst/science-planning/tools-from-the-community) that aim at removing this 1/f noise at different stages. Here, we will try a very simple algorithm to (aim to) remove it: we will subtract background pixels on each column to the entire column, hoping this alleviates this 1/f noise. To do this, we need to follow two steps: (1) identify pixels that are \"non-illuminated\" in the detector, (2) remove the median of those pixels on every column for every group and integration. Let's do this next.\n",
    "\n",
    "#### 3.6.1 Creating a background pixel mask\n",
    "\n",
    "To create a \"background pixel mask\", we will here simply work with the first segment of data --- doing the same for all segments of data together is left as an exercise to the reader. The first step to generating this mask is to find a good estimate of the \"median\" frame --- i.e., how the median or average frame looks like. A good trick for this is to use the \"last-minus-first\" frames, which as the name suggests, simply removes the last group from the first to generate an estimate of the frame for a given integration. Let's do this for NRS1 and NRS2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee298b7-accd-4d82-9bc4-2e12d904bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Last-minus-first for NRS1:\n",
    "lmf_nrs1 = uncal_nrs1[0].data[:, -1, :, :] - uncal_nrs1[0].data[:, 0, :, :]\n",
    "\n",
    "# Same for NRS2:\n",
    "lmf_nrs2 = uncal_nrs2[0].data[:, -1, :, :] - uncal_nrs2[0].data[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff92e64-d29e-4d41-97cf-8b4aa3af6739",
   "metadata": {},
   "source": [
    "Let's calculate the median of those, and see how they look-like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b001f2a7-e0aa-42dc-89f1-449e15df851e",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_lmf_nrs1 = np.nanmedian( lmf_nrs1, axis = 0 )\n",
    "median_lmf_nrs2 = np.nanmedian( lmf_nrs2, axis = 0 )\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(median_lmf_nrs1, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-10, 10)\n",
    "plt.title('Median last-minus-first frame, NRS1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(median_lmf_nrs2, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-10, 10)\n",
    "plt.title('Median last-minus-first frame, NRS2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6f6f0-1100-4252-8dd1-03e0409452ff",
   "metadata": {},
   "source": [
    "All right! These look great. Next-up, let's calculate a very simple and crude estimate of the centroid of the spectral PSF profile. To this end, let's first get a median-filtered version of this 2D image to smooth outliers out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eeedc7-ac90-4076-93e7-b65404df6877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernel size is 5-pixels in the rows and 11 in the columns --- roughly by eye how much pixels it takes to \"jump over\" outliers:\n",
    "mf_median_lmf_nrs1 = medfilt2d(median_lmf_nrs1, kernel_size = (5, 11))\n",
    "mf_median_lmf_nrs2 = medfilt2d(median_lmf_nrs2, kernel_size = (5, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2290e684-2703-411b-9730-d9b614cc8cbf",
   "metadata": {},
   "source": [
    "It's not very important that this median filtered version is great --- we just need for outliers to be smoothed out. Did we succeed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3233454b-245d-4db9-b37c-8bae4bb0854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(mf_median_lmf_nrs1, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-10, 10)\n",
    "plt.title('Median-filtered last-minus-first frame, NRS1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(mf_median_lmf_nrs2, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-10, 10)\n",
    "plt.title('Median-filtered last-minus-first frame, NRS2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80983fe2-28ea-41cd-8bd5-b7a6feed4120",
   "metadata": {},
   "source": [
    "Yes! This works. Now, to simply get the centroid of each column --- we start from column 450 to 2043 in NRS1, and from pixel 4 to 2043 in NRS2. We estimate the centroids via a simple flux-weighted sum of row-positions, and then we fit those with 2nd degree polynomials. Again, our aim is not to make this perfect, but to get a \"good enough\" representation of the trace:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9155dc2a-60c3-4864-9094-08afd9b43804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rows for the weighted sum:\n",
    "rows = np.arange(32)\n",
    "\n",
    "# All right, calculate trace positions for NRS1:\n",
    "x_nrs1 = np.arange(550, 2044, 1)\n",
    "y_nrs1 = np.zeros( len(x_nrs1) )\n",
    "for i in range(len(x_nrs1)):\n",
    "\n",
    "    y_nrs1[i] = np.sum(mf_median_lmf_nrs1[:, x_nrs1[i]] * rows) / np.sum(mf_median_lmf_nrs1[:, x_nrs1[i]])\n",
    "\n",
    "# Fit a degree 3 polynomial, extending the x-value down to 300:\n",
    "coeff = np.polyfit(x_nrs1, y_nrs1, 2)\n",
    "x_nrs1 = np.arange(300, 2044, 1)\n",
    "y_nrs1 = np.polyval(coeff, x_nrs1)\n",
    "\n",
    "# Now repeat for NRS2:\n",
    "x_nrs2 = np.arange(4, 2044, 1)\n",
    "y_nrs2 = np.zeros( len(x_nrs2) )\n",
    "for i in range(len(x_nrs2)):\n",
    "\n",
    "    y_nrs2[i] = np.sum(mf_median_lmf_nrs2[:, x_nrs2[i]] * rows) / np.sum(mf_median_lmf_nrs2[:, x_nrs2[i]])\n",
    "\n",
    "# Fit a degree 3 polynomial:\n",
    "coeff = np.polyfit(x_nrs2, y_nrs2, 2)\n",
    "y_nrs2 = np.polyval(coeff, x_nrs2)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5422eb-7cfc-40d6-9ea5-5141dd783786",
   "metadata": {},
   "source": [
    "How did our simple algorithm do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8115c5f-03f5-4294-8c99-02c7baf5c2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(median_lmf_nrs1, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "\n",
    "plt.plot(x_nrs1, y_nrs1, '-', color = 'cornflowerblue', lw = 3)\n",
    "    \n",
    "im.set_clim(-10, 10)\n",
    "plt.title('Median last-minus-first frame, NRS1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(median_lmf_nrs2, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "\n",
    "plt.plot(x_nrs2, y_nrs2, '-', color = 'cornflowerblue', lw = 3)\n",
    "    \n",
    "im.set_clim(-10, 10)\n",
    "plt.title('Median last-minus-first frame, NRS2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfb2624-9c5e-48c8-8b2b-ec2b33f8f58e",
   "metadata": {},
   "source": [
    "All right! These look great. \n",
    "\n",
    "Using these traces, we simply create a mask by masking all pixels within `r` pixels away from those traces on each column as `nan`, and all others pixels with values of `1`. The reason for this will be evident below. We will be very conservative, and take a radius `r` of 12 pixels. Let's create the masks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407ca88b-86ab-4b70-92c7-603b12edf9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define r:\n",
    "r = 12 # pixels\n",
    "\n",
    "# Create nan masks:\n",
    "background_mask_nrs1 = np.ones(median_lmf_nrs1.shape)\n",
    "background_mask_nrs2 = np.ones(median_lmf_nrs2.shape)\n",
    "\n",
    "# Mark all pixels within the trace swith 1's. First for NRS1:\n",
    "for i in range(len(x_nrs1)):\n",
    "\n",
    "    lower_bound = np.max([int(y_nrs1[i]-r), 0])\n",
    "    upper_bound = np.min([32, int(y_nrs1[i]+r)])\n",
    "    background_mask_nrs1[lower_bound:upper_bound, x_nrs1[i]] = np.nan\n",
    "\n",
    "# Repeat for NRS2:\n",
    "for i in range(len(x_nrs2)):\n",
    "\n",
    "    lower_bound = np.max([int(y_nrs2[i]-r), 0])\n",
    "    upper_bound = np.min([32, int(y_nrs2[i]+r)])\n",
    "    background_mask_nrs2[lower_bound:upper_bound, x_nrs2[i]] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d00f5da-b56c-4c71-b21b-09875b6f72f7",
   "metadata": {},
   "source": [
    "Let's see how our masks look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e690aff0-30bc-419e-9346-85c806527248",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(background_mask_nrs1, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "plt.title('Background pixel mask, NRS1')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(background_mask_nrs2, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "\n",
    "plt.title('Background pixel mask, NRS2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c74273-f88a-4db4-97e6-818b92706372",
   "metadata": {},
   "source": [
    "Excellent! Let's use those masks to handle 1/f noise next.\n",
    "\n",
    "#### 3.6.2 Removing 1/f noise at the group-level\n",
    "\n",
    "Let's now use the mask generated above to remove 1/f noise with the \"column-to-column\" subtraction algorithm described above. To this end, we simply iterate through all integrations and all groups, multiply this `nan` mask to the data, compute the nan-median (i.e., the median using only non-nan pixels --- this is why we created a `nan` mask in the first place!) of each column --- and then remove this value from each column. \n",
    "\n",
    "Let's pack this idea in a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae32a0a-402d-4e14-b361-e6f8d75d424b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_level_1f_correction(input, nanmask):\n",
    "\n",
    "    # Generate median of all columns using the nanmask:\n",
    "    median_per_column = np.nanmedian( input * nanmask, axis = 0)\n",
    "\n",
    "    # Return input minus this median per column values:\n",
    "    return input - median_per_column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefcb4d-361f-4610-a9e7-50f49b83a825",
   "metadata": {},
   "source": [
    "Now, let's do this correction on an example integration and pair of groups to see how well this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053add6e-34da-427c-9fc5-52d66287f314",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrs1_corrected_group1 = group_level_1f_correction(uncal_nrs1[0].data[10, 0, :, :], background_mask_nrs1)\n",
    "nrs1_corrected_group2 = group_level_1f_correction(uncal_nrs1[0].data[10, 1, :, :], background_mask_nrs1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeaf04f-42f6-43ae-b22f-2194d8efec42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(uncal_nrs1[0].data[10, 1, :, :] - uncal_nrs1[0].data[10, 0, :, :], \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-25,25)\n",
    "plt.title('NRS1 --- 10th integration, group 2-1 --- before 1/f correction')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(nrs1_corrected_group2 - nrs1_corrected_group1, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-25,25)\n",
    "plt.title('NRS1 --- 10th integration, group 2-1 --- after 1/f correction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70ad3c3-d2ee-47c4-b6af-90eca6e64cf5",
   "metadata": {},
   "source": [
    "Nice! How about doing it on NRS2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b9ba90-161b-49a5-8f25-cdfcb343ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrs2_corrected_group1 = group_level_1f_correction(uncal_nrs2[0].data[10, 0, :, :], background_mask_nrs2)\n",
    "nrs2_corrected_group2 = group_level_1f_correction(uncal_nrs2[0].data[10, 1, :, :], background_mask_nrs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bd19f5-08e7-4613-a071-94ba235c7ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(uncal_nrs2[0].data[10, 1, :, :] - uncal_nrs2[0].data[10, 0, :, :], \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-25,25)\n",
    "plt.title('NRS2 --- 10th integration, group 2-1 --- before 1/f correction')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,3))\n",
    "    \n",
    "im = plt.imshow(nrs2_corrected_group2 - nrs2_corrected_group1, \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-25,25)\n",
    "plt.title('NRS2 --- 10th integration, group 2-1 --- after 1/f correction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34867f6d-bd45-4336-a882-e7930032ed72",
   "metadata": {},
   "source": [
    "Very nice! Noticeable 1/f-noise reduction using a very simple algorithm. Let's now apply it to all integrations, all groups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a23f197-103e-4e35-ae97-8bea5b55e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "for i in range(nsegments):\n",
    "\n",
    "    # Apply 1/f corrections to all integrations and groups in the segment for NRS1:\n",
    "    for integration in range(uncal_nrs1[i].shape[0]):\n",
    "\n",
    "        for group in range(uncal_nrs1[i].shape[1]):\n",
    "            \n",
    "            uncal_nrs1[i].data[integration, group, :, :] = group_level_1f_correction(uncal_nrs1[i].data[integration, group, :, :], \n",
    "                                                                                     background_mask_nrs1\n",
    "                                                                                    )\n",
    "    \n",
    "    # Same, for NRS2:\n",
    "    for integration in range(uncal_nrs2[i].shape[0]):\n",
    "\n",
    "        for group in range(uncal_nrs2[i].shape[1]):\n",
    "            \n",
    "            uncal_nrs2[i].data[integration, group, :, :] = group_level_1f_correction(uncal_nrs2[i].data[integration, group, :, :], \n",
    "                                                                                     background_mask_nrs2\n",
    "                                                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc98259-ac61-4985-956d-7de273357d79",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note on 1/f correction methods</b>: For TSOs, there is some discussion in the literature about whether attempting to remove 1/f noise at the group-level, at the rate-level (i.e., after all the steps in <code>detector1</code>) or both is the way to go --- and whether simplistic algorithms like the one above are optimal. The reality is that, at the time of writing, the jury is still out on the final answer. We thus encourage readers to try different methodologies and find the one that works best for their scientific use-case. As a start, an interesting reader might, e.g., skip the above 1/f removal algorithm and simply try to remove it at the rate-level --- or perform no removal at all, and see differences in the final lightcurve precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5cafb8",
   "metadata": {},
   "source": [
    "### 3.7<font color='white'>-</font>Detecting \"jumps\" in up-the-ramp samples <a class=\"anchor\" id=\"jump\"></a>\n",
    "\n",
    "When a cosmic-ray hits JWST detectors, this impacts the up-the-ramp samples by making them \"[jump](https://www.youtube.com/watch?v=SwYN7mTi6HM)\" from one group to another. We already noted this happening above \n",
    "[when we discussed saturation](#saturation) --- a pixel was suddenly pushed above the saturation limit and the `saturation` step flagged the pixel. However, some other jumps are not as dramatic, and the data after the jump might actually be as usable as data before the jump.\n",
    "\n",
    "\n",
    "#### 3.7.1 Understanding jumps and the `jump` step\n",
    "\n",
    "To exemplify the behavior of the jumps in up-the-ramp samples, let's look at an example. Consider the behavior of pixel index `(12,1000)` in integration `67`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506bf4de-99d0-470c-a1b7-d21fabac9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.title('Pixel index (12, 1000)')\n",
    "\n",
    "group = np.arange(uncal_nrs1[0].data.shape[1])\n",
    "plt.plot( group+1, uncal_nrs1[0].data[67, :, 12, 1000], 'o-', color = 'black', mfc = 'white', label = 'Integration 67')\n",
    "plt.plot( group+1, uncal_nrs1[0].data[66, :, 12, 1000], 'o-', \n",
    "          color = 'tomato', mfc = 'white', label = 'Integration 66', alpha = 0.5)\n",
    "plt.plot( group+1, uncal_nrs1[0].data[68, :, 12, 1000], 'o-', \n",
    "          color = 'cornflowerblue', mfc = 'white', label = 'Integration 68', alpha = 0.5)\n",
    "\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16)\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 70.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bde0f8-c2ad-4e3c-ac64-7805f8083ac6",
   "metadata": {},
   "source": [
    "While the intercept of the different up-the-ramp samples is slightly different, the _slope_ (i.e., the count-rate) of it is fairly similar for integrations 66, 67 and 68. However, integration 67 shows a clear jump at group 15, likely from a cosmic ray. Let's take a look at what happened in this integration and group in the 2D spectrum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eddc0da-b2af-45b7-9974-fa3272201053",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "    \n",
    "im = plt.imshow(uncal_nrs1[0].data[66,14,:,:], \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-100,1000)\n",
    "plt.xlim(1000-5,1000+5)\n",
    "plt.ylim(12-5,12+5)\n",
    "plt.title('Integration 66, group 15')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "    \n",
    "im = plt.imshow(uncal_nrs1[0].data[67,14,:,:], \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-100,1000)\n",
    "plt.xlim(1000-5,1000+5)\n",
    "plt.ylim(12-5,12+5)\n",
    "plt.title('Integration 67, group 15')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "    \n",
    "im = plt.imshow(uncal_nrs1[0].data[68,14,:,:], \n",
    "                interpolation = 'None', aspect = 'auto', origin = 'lower')\n",
    "    \n",
    "im.set_clim(-100,1000)\n",
    "plt.xlim(1000-5,1000+5)\n",
    "plt.ylim(12-5,12+5)\n",
    "plt.title('Integration 68, group 15')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d824c640-e5a7-4292-a5c3-d53bb1ec0ff1",
   "metadata": {},
   "source": [
    "Ah! Clearly some cosmic ray hitting around pixel `(12,1001)`, with an area of about a pixel --- including pixel `(12,1000)`. Note that the `groupdq` doesn't show anything unusual so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39d9aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].groupdq[67, 14, 12, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206b1c74",
   "metadata": {},
   "source": [
    "The JWST Calibration pipeline has an algorithm that aims to detect those jumps --- and is appropriately named the `jump` step. An important consideration when running the `jump` step is that one can use multiprocessing to run the step. This can offer dramatic speed improvements when running the step, in particular on large subarrays of data. The number of cores to use can be defined by the `maximum_cores` parameter, which can be an integer number or `all`, which will use all available cores. \n",
    "\n",
    "Let's run the step using all cores (if not already ran):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eea42c-1165-42dd-a46f-104fe3a40ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "\n",
    "for i in range(nsegments):\n",
    "\n",
    "    if not os.path.exists('NRS2_jumpstep_seg003.fits'):\n",
    "\n",
    "        # Apply the jump step to NRS1:\n",
    "        uncal_nrs1[i] = calwebb_detector1.jump_step.JumpStep.call(uncal_nrs1[i], maximum_cores = 'all')\n",
    "    \n",
    "        # Same, for NRS2:\n",
    "        uncal_nrs2[i] = calwebb_detector1.jump_step.JumpStep.call(uncal_nrs2[i], maximum_cores = 'all')\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Load NRS1 jump-step products:\n",
    "        uncal_nrs1[i] = datamodels.RampModel('NRS1_jumpstep_seg00'+str(i+1)+'.fits')\n",
    "    \n",
    "        # Same, for NRS2:\n",
    "        uncal_nrs2[i] = datamodels.RampModel('NRS2_jumpstep_seg00'+str(i+1)+'.fits')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d989a3-62f1-4ab2-ba3d-73146bea5396",
   "metadata": {},
   "source": [
    "It's not too obvious from the messages in the pipeline what happened, but the algorithm was used to _detect_ jumps, and these are added as new data-quality flags in the `groupdq`. Let's see what happened with the pixel identified by eye above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341559a9-3240-4f14-ab87-1d7e72427971",
   "metadata": {},
   "outputs": [],
   "source": [
    "uncal_nrs1[0].groupdq[67, 14, 12, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b64bf86",
   "metadata": {},
   "source": [
    "Aha! It changed. What does this mean? Let's repeat the trick we learned with the `saturation` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ed49d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodels.dqflags.dqflags_to_mnemonics(uncal_nrs1[0].groupdq[67, 14, 12, 1000], \n",
    "                                        mnemonic_map = datamodels.dqflags.pixel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a487a84",
   "metadata": {},
   "source": [
    "Nice! We now have a flag that identifies when a jump detection happened. \n",
    "\n",
    "#### 3.7.2 Jump rates per integration\n",
    "\n",
    "For fun, let's use the `groupdq` changes to figure out how many jumps happened per integration on this first segment of data by simple differencing with the products from the previous step, the `dark_current` step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc19d0-c707-42c0-9873-97cb724dfca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array that will store the number of jumps per integration:\n",
    "njumps = np.zeros(uncal_nrs1[0].groupdq.shape[0])\n",
    "\n",
    "# Iterate through integrations counting how many pixels changed in all groups:\n",
    "for integration in range(uncal_nrs1[0].groupdq.shape[0]):\n",
    "\n",
    "    groupdq_difference = uncal_nrs1[0].groupdq[integration, :, :, :] - darkcurrent_results.groupdq[integration, :, :, :]\n",
    "    wherejumps = np.where(groupdq_difference != 0.)\n",
    "    njumps[integration] = len(wherejumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b238f93-18ae-4453-9e8b-9962e87cb075",
   "metadata": {},
   "source": [
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20e518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.title('Number of jumps on the first segment of data for NRS1')\n",
    "\n",
    "integrations = np.arange(uncal_nrs1[0].groupdq.shape[0]) + 1\n",
    "plt.plot( integrations, njumps, 'o-', color = 'black', mfc = 'white' )\n",
    "\n",
    "plt.xlabel('Integration', fontsize = 16)\n",
    "plt.ylabel('Number of jumps', fontsize = 16)\n",
    "plt.xlim(0.5, uncal_nrs1[0].groupdq.shape[0] + 0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b104d5-0b46-4ddb-a0ec-afa10717004f",
   "metadata": {},
   "source": [
    "Very interesting! Per integration, it seems on the order of ~5,000 average jumps are detected. Each integration has (ngroups) x (number of pixels) =  70 x 32 x 2048 = 4587520 opportunities for jumps to appear, so this means an average rate of (detected events) / (total opportunities) = 0.11% per integration for this particular segment, detector and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50a6cf0-3433-40dc-90b7-b579ba115f10",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"> <b>Note on the effectiveness of the <code>jump</code> detection step</b>: The <code>jump</code> detection step uses, by default, <a href=\"https://jwst-pipeline.readthedocs.io/en/latest/jwst/jump/description.html#multiprocessing\">a two-point difference method</a> that relies on appropriate knowledge of the read-noise of the detector. In some cases, this might be significantly off (or <code>detector1</code> corrections might not be optimal as to leave significant detector effects) such that the algorithm might be shown to be too aggressive. Similarly, the algorithm relies on a decent amount of groups in the integration to work properly (larger than about 5). It is, thus, important to try different parameters to identify jumps in a given dataset and study their impact on the final products. One of the most important parameters is the <code>rejection_threshold</code>. The default value is <code>4</code>, but TSO studies in the literature have sometimes opted for more conservative values (typically larger than 10). For this particular dataset, which has a large number of groups (70), the default value works well, but it might not be optimal nor be the best for other datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3afc925-f0b2-49dc-80af-ec9ef301f5a8",
   "metadata": {},
   "source": [
    "Before moving to the next step, we showcase one additional function from the `datamodels` which allows to save products to files --- the `save` function. We use it to save our `jump` step products for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2785e5-b7fc-4301-9985-d7fd67153495",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('NRS2_jumpstep_seg003.fits'):\n",
    "    \n",
    "    nsegments = 3\n",
    "\n",
    "    for i in range(nsegments):\n",
    "\n",
    "        # Save jump step NRS1 products:\n",
    "        uncal_nrs1[i].save('NRS1_jumpstep_seg00'+str(i+1)+'.fits')\n",
    "    \n",
    "        # Same, for NRS2:\n",
    "        uncal_nrs2[i].save('NRS2_jumpstep_seg00'+str(i+1)+'.fits')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea1fad",
   "metadata": {},
   "source": [
    "### 3.8<font color='white'>-</font>Fitting ramps with the `ramp_fit` step <a class=\"anchor\" id=\"rampfit\"></a>\n",
    "\n",
    "The last step of `detector1` is the `ramp_fit` step. This step does something that might _appear_ to be quite simple, but that in reality it's not as trivial as it seems to be: fit a line and get the associated uncertainties to the up-the-ramp samples. The reason why this is not straightforward to do is because samples up-the-ramp are correlated. That is, because signal is accumulated up-the-ramp, group number 2 has a non-zero covariance with group number 1, and so on. \n",
    "\n",
    "#### 3.8.1 Applying the `ramp_fit` step to JWST data\n",
    "\n",
    "The JWST Calibration pipeline algorithm performs a sensible weighting of each group in order to account for that correlation when fitting a slope on the samples. Let's run this step, and save the products in files as we go, so we can use them for the next notebook. Note that as in the `jump` step, we can also run this step via multi-processing --- and we do just that below (if not ran already):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d70ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsegments = 3\n",
    "ramps_nrs1 = []\n",
    "ramps_nrs2 = []\n",
    "\n",
    "for i in range(nsegments):\n",
    "\n",
    "    if not os.path.exists('NRS2_jumpstep_seg003_1_rampfitstep.fits'):\n",
    "        \n",
    "        # Apply the rampfit step to NRS1:\n",
    "        ramps_nrs1.append( calwebb_detector1.ramp_fit_step.RampFitStep.call(uncal_nrs1[i], \n",
    "                                                                            maximum_cores = 'all', \n",
    "                                                                            save_results=True)\n",
    "                         )\n",
    "    \n",
    "        # Same, for NRS2:\n",
    "        ramps_nrs2.append( calwebb_detector1.ramp_fit_step.RampFitStep.call(uncal_nrs2[i], \n",
    "                                                                            maximum_cores = 'all',\n",
    "                                                                            save_results=True) \n",
    "                         )\n",
    "\n",
    "    else:\n",
    "\n",
    "        ramps_nrs1.append( [ datamodels.open('NRS1_jumpstep_seg00'+str(i+1)+'_0_rampfitstep.fits'), \n",
    "                             datamodels.open('NRS1_jumpstep_seg00'+str(i+1)+'_1_rampfitstep.fits')\n",
    "                           ] \n",
    "                         )\n",
    "\n",
    "        ramps_nrs2.append( [ datamodels.open('NRS2_jumpstep_seg00'+str(i+1)+'_0_rampfitstep.fits'), \n",
    "                             datamodels.open('NRS2_jumpstep_seg00'+str(i+1)+'_1_rampfitstep.fits')\n",
    "                           ] \n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96724249",
   "metadata": {},
   "source": [
    "All right! First of all, note the products of this step for TSO's are actually a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b214adf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ramps_nrs1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159fe870",
   "metadata": {},
   "source": [
    "The data associated with the zeroth element of this list (`ramps_nrs1[0][0].data`) has dimensions equal to the size of the frames (rows and columns). The first element (`ramps_nrs1[0][1].data`), has three dimensions, the same as the zeroth but for each integration. We usually refer to this latter product as the `rateints` product --- i.e., the rates per integration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_nrs1[0][0].data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990028f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_nrs1[0][1].data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ae6b9d",
   "metadata": {},
   "source": [
    "To familiarize ourselves with these products, let's plot the rates of the 10th integration for NRS1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.title('NRS1 data; rates for integration 10')\n",
    "im = plt.imshow(ramps_nrs1[0][1].data[10, :, :], aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(-1,1)\n",
    "\n",
    "plt.colorbar(label = 'Counts/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7fcc50-a114-4c2d-a487-a9f5e493b912",
   "metadata": {},
   "source": [
    "In case you were unsure of the units in the colorbar, you can double-check them through the `datamodels` themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937e8653-3c4c-41ad-9602-fdb034f0db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_nrs1[0][1].search('unit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998289d",
   "metadata": {},
   "source": [
    "These rates look very pretty! What about the same for NRS2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27110f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,3))\n",
    "\n",
    "plt.title('NRS2 data; rates for integration 10')\n",
    "im = plt.imshow(ramps_nrs2[0][1].data[10, :, :], aspect = 'auto', origin = 'lower')\n",
    "im.set_clim(-1,1)\n",
    "\n",
    "plt.colorbar(label = 'Counts/s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc22644d",
   "metadata": {},
   "source": [
    "These rates look _very_ good. \n",
    "\n",
    "\n",
    "#### 3.8.2 Linking rates and up-the-ramp samples\n",
    "\n",
    "Do these rates match the slopes observed on real up-the-ramp samples? Let's check integration 68 of pixel (`12,1000`), which we already encountered above when we studied the `jump`-step. \n",
    "\n",
    "We first need to figure out the group-time, as the rates are in counts per second. This is easy to obtain from the `datamodels` themselves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e352eb7d-a5a8-4354-b444-34a9187cbc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "ramps_nrs1[0][1].search('group_time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1642aa5-d009-41f1-b4c6-303851e31aa2",
   "metadata": {},
   "source": [
    "All right! Group-time is 0.902 seconds. With this, let's plot the up-the-ramp samples, and the slope on top estimated by the ramp-fitting algorithm along with the _error_ on the ramp, which can be obtained from the `err` array in the `datamodel` for the rates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7090b6fd-fe1f-419d-9d66-9b5e3d303b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "\n",
    "plt.title('Up-the-ramp samples, integration 68; pixel (12, 1000)')\n",
    "\n",
    "group = np.arange(uncal_nrs1[0].data.shape[1])\n",
    "plt.plot( group+1, uncal_nrs1[0].data[68, :, 12, 1000], 'o-', \n",
    "          color = 'tomato', mfc = 'white')\n",
    "\n",
    "slope = ramps_nrs1[0][1].data[68, 12, 1000] # In counts per second\n",
    "group_time = ramps_nrs1[0][1].meta.exposure.group_time # Group time in seconds\n",
    "\n",
    "# Estimate rate up-the-ramp:\n",
    "plt.plot( group+1, uncal_nrs1[0].data[68, :, 12, 1000][0] + (group+1) * group_time * slope, color = 'black')\n",
    "\n",
    "plt.xlabel('Group number', fontsize = 16)\n",
    "plt.ylabel('Counts', fontsize = 16)\n",
    "plt.legend()\n",
    "plt.xlim(0.5, 70.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a2471-550c-4cd2-9988-998e34c82055",
   "metadata": {},
   "source": [
    "That looks pretty good!\n",
    "\n",
    "#### 3.8.3 Crude rate-based white-light lightcurves\n",
    "\n",
    "Before ending this notebook, let's get a very quick, crude, lightcurve from NRS1 and NRS2, so we can see that the transit event is actually in our data. To this end, let's first stitch all ramps together in a single array for NRS1 and NRS2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6ad5aff-2eff-4d81-85f3-0444a52e6bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrs1_rateints = np.vstack(( ramps_nrs1[0][1].data, ramps_nrs1[1][1].data ))\n",
    "nrs1_rateints = np.vstack(( nrs1_rateints, ramps_nrs1[2][1].data ))\n",
    "\n",
    "nrs2_rateints = np.vstack(( ramps_nrs2[0][1].data, ramps_nrs2[1][1].data ))\n",
    "nrs2_rateints = np.vstack(( nrs2_rateints, ramps_nrs2[2][1].data ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19600b9b-53f6-4d89-9848-dc0c61c4d2d2",
   "metadata": {},
   "source": [
    "And now let's get the sum accross the detector for the rateint products:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20492a93-b10a-4f00-8a4d-0d9629f01e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_nrs1 = np.nansum( nrs1_rateints, axis = (1,2) )\n",
    "lc_nrs2 = np.nansum( nrs2_rateints, axis = (1,2) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca98ceaa-34c1-4bac-97b2-37c47a6864ef",
   "metadata": {},
   "source": [
    "Let's now plot some crude, normalized lightcurves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b469a55d-b882-4c9e-b339-ad9e92bec4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,2))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Crude white-light lightcurve: NRS1')\n",
    "\n",
    "plt.plot( lc_nrs1 / np.nanmedian(lc_nrs1[:100]), 'o-', \n",
    "          color = 'cornflowerblue', mfc = 'white', ms = 2)\n",
    "\n",
    "\n",
    "plt.xlabel('Integration index', fontsize = 16)\n",
    "plt.ylabel('Relative flux', fontsize = 16)\n",
    "plt.xlim(0.5, len(lc_nrs1)+0.5)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.title('Crude white-light lightcurve: NRS2')\n",
    "\n",
    "plt.plot( lc_nrs2 / np.nanmedian(lc_nrs2[:100]), 'o-', \n",
    "          color = 'tomato', mfc = 'white', ms = 2)\n",
    "\n",
    "plt.xlabel('Integration index', fontsize = 16)\n",
    "plt.xlim(0.5, len(lc_nrs1)+0.5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70c6b40",
   "metadata": {},
   "source": [
    "Et voilÃ ! This is by all means a very crude white-light lightcurve, but this does showcase that our processing is in the right direction. The proper spectroscopic extraction and lightcurve generation will be done in the companion notebook!\n",
    "\n",
    "4.<font color='white'>-</font>Final words <a class=\"anchor\" id=\"final-words\"></a>\n",
    "------------------\n",
    "\n",
    "It is important to note that the JWST pipeline as it is right now will and currently is changing as we learn more about on-sky data. The number of people behind this effort is large, and as such I would like to take the opportunity to thank the entire JWST team behind this notebook, that through testing (with actual detector data, through simulations, etc.) and discussions made this product possible. \n",
    "\n",
    "For this particular effort of writing the notebook, I would like to thank the JWST Time-Series Observations Working Group at STScI, especially to Mike Reagan, Stephan Birkmann, Nikolay Nikolov, Arpita Roy, LoÃ¯c Albert and Leonardo Ubeda among others. To the NIRCam IDT members, including Everett Schlawin, Jarron Leisenring, Thomas Greene and Thomas Beatty with which a ton of discussion has been ongoing on different topics touched upon here. To the ERS Transiting Exoplanet team who have provided several venues for discussion and community input. To the JWST team behind the pipeline and the mission itself, including and in no particular order Anton Koekemoer, Alicia Canipe, Jeff Valenti, Karl Gordon, Bryan Hilbert and Joseph Filippazzo. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
